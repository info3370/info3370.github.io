---
output: beamer_presentation
header-includes:
  \usepackage{tikz}
  \usetikzlibrary{arrows,shapes.arrows,positioning,shapes,patterns,calc}
  \newcommand\bblue[1]{\textcolor{blue}{\textbf{#1}}}
  \usepackage{xcolor}
  \definecolor{seagreen}{RGB}{46, 139, 87}
---

#

\begin{tikzpicture}[x = \textwidth, y = \textheight]
\node at (0,0) {};
\node at (1,1) {};
\node[anchor = north west, align = left, font = \huge] at (0,.9) {Studying\\Social Inequality\\with Data Science};
\node[anchor = north east, align = right] (number) at (1,.9) {INFO 3370 / 5371\\Spring 2024};
\node[anchor = north, font = \Large, align = left] at (.5,.5) {\bblue{Predicting life outcomes}\\Results of the PSID Income Prediction Challenge};
\end{tikzpicture}


# Learning goals for today

By the end of class, you will be able to

- know who had the best predictions!
- reason about predictability of life outcomes

# Equality Opportunity and Prediction

**Possible claim**

To the degree that we can predict life outcomes,

people do not have equal opportunity

# Equality Opportunity and Prediction

\begin{tikzpicture}[x = 3in, y = 2in]
\node[anchor = east, rotate = 270, font = \footnotesize] at (.15,1) {Parent Income};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.25,1) {Grandparent Income};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.35,1) {Race};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.45,1) {Sex};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.55,1) {Grandparent Education};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.65,1) {Parent Education};
\node[anchor = east, rotate = 270, font = \footnotesize] at (.75,1) {Respondent Education};
\node[anchor = east, font = \footnotesize] at (.1,.95) {Case 1};
\node[anchor = east, font = \footnotesize] at (.1,.85) {Case 2};
\node[anchor = east, font = \footnotesize] at (.1,.75) {Case 3};
\node[anchor = east, font = \footnotesize] at (.1,.65) {Case 4};
\node[anchor = east, font = \footnotesize] at (.1,.55) {Case 5};
\draw[step=.1,black,thin] (0.1,.5) grid (.8,1); 
\node at (.5,1) {};
\node[anchor = east, rotate = 270, font = \footnotesize] at (1.15,1) {Respondent Income};
\draw[step=.1,black,thin] (1.1,.5) grid (1.2,1);
\draw[thin] (1.2,.5) -- (1.2,1);
\draw[->, thick] (.85,.65) -- node[midway, above, font = \footnotesize, align = center] {Learn a\\prediction\\function} (1.05,.65);
% Holdout set
\draw[step=.1,black,thin] (0.1,.1) grid (.8,.4);
\draw[step=.1,black,thin] (1.1,.1) grid (1.2,.4);
\draw[thin] (1.2,.1) -- (1.2,.4);
\node at (1.15,.35) {?};
\node at (1.15,.25) {?};
\node at (1.15,.15) {?};
\node[anchor = east, font = \footnotesize] at (.1,.35) {Case 6};
\node[anchor = east, font = \footnotesize] at (.1,.25) {Case 7};
\node[anchor = east, font = \footnotesize] at (.1,.15) {Case 8};
\draw[->, thick] (.85,.25) -- node[midway, above, font = \footnotesize, align = center] {Predict for\\new cases} (1.05,.25);
% Label the two
\node[rotate = 90, font = {\footnotesize\bf}, blue] at (-.1,.75) {{Learning} Set};
\node[rotate = 90, font = {\footnotesize\bf}, seagreen] at (-.1,.25) {{Holdout} Set};
\node at (-.05,0) {};
\end{tikzpicture}

# The model selection problem

```{r, echo = F, message = F, comment = F}
library(tidyverse)
library(rsample)
set.seed(14850)
```

In supervised machine learning, the goal is to

- learn patterns in the available data
- predict outcomes for previously unseen cases

```{r, echo = F, fig.height = 2.5, dpi = 300}
p <- ggplot() +
  theme_void() +
  # Fix the canvas
  annotate(geom = "point", x = 1.25, y = 4.2, color = "white") +
  annotate(geom = "point", x = 5, y = 2, color = "white") +
  # Predictor box
  annotate(geom = "text", x = 1, y = 4, vjust = -1, label = "Predictor Variables", fontface = "bold") +
  annotate(geom = "text", x = 0, y = 2, vjust = -1, label = "Cases", fontface = "bold", angle = 90) +
  annotate(geom = "rect", xmin = 0, xmax = 2, ymin = 0, ymax = 4, fill = "white", color = "black") +
  # Learning set box  
  annotate(geom = "rect", xmin = 3, xmax = 4, ymin = 2, ymax = 4, fill = "white", color = "black") +
  annotate(geom = "text", x = 3.5, y = 4, vjust = -1, label = "Outcomes", fontface = "bold") +
  annotate(geom = "text", x = 3.5, y = 3, label = "Learning\nSet") +
  # Holdout set box
  annotate(geom = "rect", xmin = 3, xmax = 4, ymin = 0, ymax = 2, fill = "darkgray", color = "black") +
  annotate(geom = "text", x = 3.5, y = 1, label = "Holdout\nSet", color = "white") +
  # What is available to data analyst
  annotate(geom = "text", x = 4.25, y = 3, hjust = 0, label = "Available\nto Data Analyst", size = 3.2) +
  annotate(geom = "text", x = 4.25, y = 1, hjust = 0, label = "Never Available\nto Data Analyst", size = 3.2) +
  annotate(geom = "segment", linewidth = .3,
           arrow = arrow(length = unit(.05,"in")),
           x = 4.2, xend = 4.1, 
           y = c(.7,1.3,2.7,3.3),
           yend = c(.55,1.45,2.55,3.45)) +
  # Learning
  annotate(geom = "segment", x = 2.2, xend = 2.8, y = 3, yend = 3, arrow = arrow(length = unit(.05,"in"))) +
  annotate(geom = "text", fontface = "bold", x = 2.5, y = 3.1, vjust = 0, label = "Learn") +
  annotate(geom = "text", x = 2.5, y = 2.9, vjust = 1, label = "Discover\nPatterns", size = 3.2) +
  # Task
  annotate(geom = "segment", x = 2.2, xend = 2.8, y = 1, yend = 1, arrow = arrow(length = unit(.05,"in"))) +
  annotate(geom = "text", fontface = "bold", x = 2.5, y = 1.1, vjust = 0, label = "Task") +
  annotate(geom = "text", x = 2.5, y = .9, vjust = 1, label = "Predict for\nNew Cases", size = 3.2)
print(p)
```

<!-- \pause -->
<!-- How do we know which method will do this well? -->

# The model selection problem

When a task involves unseen data,

mimic the task with data we have

# The model selection problem

```{r, echo = F, fig.height = 2.5, dpi = 300}
print(p)
```

# The model selection problem

```{r, echo = F, fig.height = 2.5, dpi = 300}
p2 <- p +
  # Block things to write over
  annotate(geom = "rect", xmin = 2.1, xmax = 2.9, ymin = 2, ymax = 4, fill = "white", color = "white") +
  annotate(geom = "rect", xmin = 3.1, xmax = 3.9, ymin = 2.1, ymax = 3.9, fill = "white", color = "white") +
  # Train set
  annotate(geom = "rect", xmin = 3, xmax = 4, ymin = 3, ymax = 4, fill = "seagreen4", alpha = .8) +
  annotate(geom = "text", x = 3.5, y = 3.5, color = "white", label = "Train Set") +
  # Test set
  annotate(geom = "rect", xmin = 3, xmax = 4, ymin = 2, ymax = 3, fill = "blue", alpha = .8) +
  annotate(geom = "text", x = 3.5, y = 2.5, color = "white", label = "Test Set") +
  # Estimation
  annotate(geom = "segment", x = 2.2, xend = 2.8, y = 3.5, yend = 3.5, arrow = arrow(length = unit(.05,"in"))) +
  annotate(geom = "text", fontface = "bold", x = 2.5, y = 3.6, vjust = 0, label = "Estimate") +
  annotate(geom = "text", x = 2.5, y = 3.4, vjust = 1, label = "Discover Patterns", size = 3.2) +
  # Evaluation
  annotate(geom = "segment", x = 2.2, xend = 2.8, y = 2.5, yend = 2.5, arrow = arrow(length = unit(.05,"in"))) +
  annotate(geom = "text", fontface = "bold", x = 2.5, y = 2.6, vjust = 0, label = "Evaluate") +
  annotate(geom = "text", x = 2.5, y = 2.4, vjust = 1, label = "Select Model", size = 3.2)
print(p2)
```

<!-- # Sample split in R -->

<!-- 1. Split `learning` into `train` and `test` -->
<!-- 2. Learn candidate function in `train` -->
<!-- 3. Evaluate predictive performance in `test` -->
<!-- 4. Use the best one to predict in `holdout` -->

# Prepare environment

```{r, eval = F}
library(tidyverse)
library(rsample)
set.seed(14850)
```

# Load data

```{r, message = F, warning = F, eval = F}
learning <- read_csv("learning.csv")
holdout_public <- read_csv("holdout_public.csv")
```
```{r, message = F, warning = F, echo = F}
learning <- read_csv("../../data_raw/income_challenge/for_students/learning.csv")
holdout_public <- read_csv("../../data_raw/income_challenge/for_students/holdout_public.csv")
```

# Create a train-test split within `learning`

Using the `rsample` package,

```{r}
split <- learning |>
  initial_split(prop = 0.5)
```  

#

```{r, echo = F, fig.height = 2.5, dpi = 300}
print(p2)
```

# Learn candidates in the train set

<!-- OLS with predictors: -->

<!-- 1. parent income -->
<!-- 2. parent income + race + sex -->
<!-- 3. parent income $\times$ race $\times$ sex -->

```{r}
candidate_1 <- lm(
  g3_log_income ~ g2_log_income,
  data = training(split)
)
candidate_2 <- lm(
  g3_log_income ~ g2_log_income + race + sex,
  data = training(split)
)
candidate_3 <- lm(
  g3_log_income ~ g2_log_income * race * sex,
  data = training(split)
)
```

# Learn candidates in the train set

```{r, echo = F}
training(split) %>%
  mutate(candidate_1 = predict(candidate_1),
         candidate_2 = predict(candidate_2),
         candidate_3 = predict(candidate_3)) %>%
  pivot_longer(cols = starts_with("candidate"),
               names_to = "model", 
               values_to = "yhat") %>%
  mutate(model = case_when(model == "candidate_1" ~ "Income",
                           model == "candidate_2" ~ "Income + Race + Sex",
                           model == "candidate_3" ~ "Income x Race x Sex")) %>%
  rename(Sex = sex, Race = race) %>%
  ggplot(aes(x = g2_log_income, color = Race, linetype = Sex, y = yhat)) +
  geom_line() +
  facet_wrap(~model, ncol = 3) +
  xlab("Parent Log Income") +
  ylab("Predicted Respondent Log Income") +
  theme_bw() +
  theme(text = element_text(size = 20))
```

<!-- # 4. Evaluate predictive performance on the test set -->

```{r, echo = F}
fitted <- testing(split) %>%
  mutate(candidate_1 = predict(candidate_1, 
                               newdata = testing(split)),
         candidate_2 = predict(candidate_2, 
                               newdata = testing(split)),
         candidate_3 = predict(candidate_3, 
                               newdata = testing(split))) %>%
  pivot_longer(cols = starts_with("candidate"),
               names_to = "model", 
               values_to = "yhat")
```

# Evaluate performance on the test set. Choose a model

```{r}
fitted |>
  group_by(model) |>
  mutate(error = g3_log_income - yhat) |>
  mutate(squared_error = error ^ 2) |>
  summarize(mse = mean(squared_error))
```

<!-- # 4. Evaluate performance on the test set -->

<!-- ```{r, echo = F} -->
<!-- training(split) %>% -->
<!--   # Make predictions from the models -->
<!--   mutate(candidate_1 = predict(candidate_1), -->
<!--          candidate_2 = predict(candidate_2), -->
<!--          candidate_3 = predict(candidate_3)) %>% -->
<!--   # Pivot longer so we can summarize them all in one line -->
<!--   pivot_longer(cols = starts_with("candidate"), -->
<!--                names_to = "model", values_to = "yhat") %>% -->
<!--   group_by(model) %>% -->
<!--   mutate(error = g3_log_income - yhat) %>% -->
<!--   mutate(squared_error = error ^ 2) %>% -->
<!--   summarize(train_set_mse = mean(squared_error)) %>% -->
<!--   left_join( -->
<!--     fitted %>% -->
<!--   group_by(model) %>% -->
<!--   # Calculate prediction error -->
<!--   mutate(error = g3_log_income - yhat) %>% -->
<!--   # Calculate squared prediction error -->
<!--   mutate(squared_error = error ^ 2) %>% -->
<!--   # Calculate mean squared error -->
<!--   summarize(test_set_mse = mean(squared_error)), -->
<!--   by = "model" -->
<!--   ) %>% -->
<!--   mutate(model = case_when(model == "candidate_1" ~ "Income", -->
<!--                            model == "candidate_2" ~ "Income + Race + Sex", -->
<!--                            model == "candidate_3" ~ "Income x Race x Sex")) -->
<!-- ``` -->

# 

```{r, echo = F, fig.height = 2.5, dpi = 300}
print(p2)
```


# Apply your chosen model

Learn in the full learning set
```{r}
chosen <- lm(
  g3_log_income ~ g2_log_income + 
    race + sex,
  data = learning
)
```

Predict for the holdout set
```{r}
predicted <- holdout_public %>%
  mutate(
    predicted = predict(
      chosen, 
      newdata = holdout_public
    )
  )
```

# Summary


```{r, fig.height = 2.5, echo = F}
print(p)
```
```{r, fig.height = 2.5, echo = F}
print(p2)
```

# Your submissions

- 21 submissions
- 20 submissions predicting for all holdout cases
- 17 submissions with non-missing predictions
- 14 submissions by unique teams

# 

![](performance_all.pdf)

<!-- #  -->

<!-- ![](performance_5.pdf) -->

<!-- #  -->

<!-- ![](performance_4.pdf) -->

<!-- #  -->

<!-- ![](performance_3.pdf) -->

<!-- #  -->

<!-- ![](performance_2.pdf) -->

<!-- #  -->

<!-- ![](performance_1.pdf) -->

#

$$R^2 = 1 - \frac{\text{MSE}_\text{Model}}{\text{MSE}_\text{No Model}}$$

- score of 1 = perfect! $\text{MSE}_\text{Model} = 0$
- score of 0 = no better than no model at all

# 

![](r2_01.pdf)

How would you make sense of this?

#

our exercise was a particular case

of a broader research project

# 

\href{https://doi.org/10.1073/pnas.1915006117}{\includegraphics[width = \textwidth, trim = {0 6.5in 0 .6in}, clip]{pnas_page1}}

# 

\includegraphics<1>[height=0.8\textheight]{ff_design_public_b9}
\includegraphics<2>[height=0.8\textheight]{ff_design_public2}

#

Six age 15 outcomes:

- GPA
- Material Hardship
- Grit
- Evicted
- Job training
- Job loss

#

\begin{center}
\includegraphics[width=\textwidth]{ffc_design_matrix_ml}
\end{center}

#

441 registered participants
\begin{itemize}
\item social scientists and data scientists
\item undergraduates, grad students, and professionals
\item many working in teams
\end{itemize}

#

\begin{center}
How did they do?
\end{center}

#

\centering
\begin{tikzpicture}[x = \textwidth, y = \textheight]
\node<1>[anchor = north] at (.5,1) {\includegraphics[width=\textwidth]{image0}};
\draw<1>[fill = white, color = white] (.2,.9) rectangle (.85,.95);
\node<1> at (.55,.5) {$R^2_\text{Holdout} = 1 - \frac{\sum_{i\in\text{Holdout}} \left(y_i - \hat{y}_i\right)^2}{\sum_{i\in\text{Holdout}} \left(y_i - \bar{y}_\text{Training}\right)^2}$};
\node<2-6>[anchor = north] at (.5,1) {\includegraphics[width=\textwidth]{image1}};
\node<3>[draw, rounded corners, fill = white] at (.63,.55) {\includegraphics[height = .5\textheight]{gpa_best_scatter}};
\node<4>[draw, rounded corners, fill = white] at (.63,.55) {\includegraphics[height = .5\textheight]{gpa_best_scatter_diagonal}};
\node<5>[draw, rounded corners, fill = white] at (.63,.55) {\includegraphics[height = .5\textheight]{gpa_best_scatter_horizontal}};
\node<7>[anchor = north] at (.5,1) {\includegraphics[width=\textwidth]{image1a}};
\end{tikzpicture}

#

Lundberg et al. 2024.

[\textcolor{blue}{The origins of unpredictability in life outcome prediction tasks}](https://doi.org/10.48550/arXiv.2310.12871)

#

\includegraphics[width = \textwidth]{task}

#

In-depth, qualitative interviews
\begin{itemize}
\item 73 respondents in 40 families
\item Separate interviews with the youth and primary caregiver
\item Life history of the youth from birth to the interview ($\approx$ age 18)
\end{itemize}
\begin{center}
\includegraphics[width = .8\textwidth]{respondents}
\end{center}

#

\includegraphics[width = .8\textwidth]{origins}

# Irreducible error

\includegraphics[width = \textwidth, trim = 2.2in 4.5in 0 0, clip]{irreducible}

# Irreducible error: Unmeasurable features

Unmeasurable features occur after the feature observation window \pause
\begin{itemize}
\item Bella: A lasting event \pause
\begin{itemize}
\item after age 9, her father died \pause
\item high school went off course
\end{itemize} \pause
\item Charles: A fleeting event \pause
\begin{itemize}
\item online high school \pause
\item worked in the basement for one semester \pause
\item video games = bad grades that semester
\end{itemize}
\end{itemize}

# Irreducible error: Unmeasurable features

\includegraphics[width = \textwidth, trim = 2.2in 6.7in 0 0, clip]{irreducible}\\
\includegraphics[width = \textwidth, trim = 2.2in 2.9in 0 2.4in, clip]{irreducible}

# Irreducible error: Unmeasured features
\pause
Lola's social network \pause
\begin{itemize}
\item elderly neighbor got Lola ready for school each day \pause
\item grandparents remodeled the basement to house Lola \pause
\item aunt employed Lola's mother in a family business \pause
\end{itemize}
Predicted GPA: 3.04\hfill Actual GPA: 3.75

# Irreducible error: Unmeasured features
\includegraphics[width = \textwidth, trim = 2.2in 6.7in 0 0, clip]{irreducible}\\
\includegraphics[width = \textwidth, trim = 2.2in 1.3in 0 4.1in, clip]{irreducible}

# Irreducible error: Imperfectly measured features

\pause

\includegraphics[width = \textwidth]{howclose} \pause

A daughter told us about her "not very close" mother \pause

- kicked her out of the house and called police
- mother: ``you better start treating me better, because I might not live that long.''
- daughter: ``I couldn’t even focus in class...I was shaking.'' 

Outcome: Failed 8th grade. Low GPA. Dropped out.

# Irreducible error: Imperfectly measured features

\includegraphics[width = \textwidth, trim = 2.2in 6.7in 0 0, clip]{irreducible}\\
\includegraphics[width = \textwidth, trim = 2.2in 0in 0 5.65in, clip]{irreducible}

#

\includegraphics[width = \textwidth, trim = 0 6.7in 0 0, clip]{irreducible}\\
\includegraphics[width = \textwidth, trim = 0 0 0 2.4in, clip]{irreducible}


#

\includegraphics[width = \textwidth]{origins}


#

DISCUSSION


# Generalizing to other life outcome prediction tasks
\includegraphics[width = \textwidth]{growth}

# Implications for policy
\pause

\begin{itemize}
\item life outcome predictions may be inaccurate \pause
\begin{itemize}
\item if generated by algorithms
\item if generated by humans \pause
\end{itemize}
\item from accuracy to impact evaluations
\end{itemize}

# Implications for science
\pause

\begin{itemize}
\item old goal: between-group variability
\begin{itemize}
\item how means vary across groups
\end{itemize} \pause
\item new goal: within-group variability
\begin{itemize}
\item how variances vary across groups
\end{itemize} \pause
\item more work to better understand unpredictability
\begin{itemize}
\item empirical estimates
\item formal models
\end{itemize}
\end{itemize}

# Learning goals for today

By the end of class, you will be able to

- know who had the best predictions!
- reason about predictability of life outcomes


