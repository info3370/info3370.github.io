{
  "hash": "56e540e05235b895a39572d8c691548b",
  "result": {
    "markdown": "---\ntitle: \"Sample Splitting\"\n---\n\n\nA predictive model is an input-output function:\n\n- the input is a set of features $\\vec{x}$\n- the output is a predicted outcome $\\hat{y}$\n\nThe performance of a model can be measured by how well the output $\\hat{y}$ corresponds to the true outcome $y$. This page considers three ways to assess the performance of a model.\n\n1. In-sample prediction\n     - learn a model using our sample\n     - predict in that same sample\n     - evaluate mean squared error\n2. Out-of-sample prediction\n     - learn a model in one sample\n     - predict in a new sample from the same data generating process\n     - evaluate mean squared error in the new sample\n3. Split-sample prediction\n     - split our sampled cases randomly into training and testing sets\n     - learn a model in the training set\n     - predict in the testing set\n     - evaluate mean squared error in the testing set\n     \nOften, the purpose a predictive model is to accomplish out-of-sample prediction (2). But when learning the model, often only one sample is available. Therefore, evaluation by split-sample prediction (3) is often desirable because it most closely mimics this task.\n\n## Simulated setting\n\nWhen a model is evaluated by in-sample prediction, there is a danger: even if the features have no predictive value in the population, a model might discover patterns that exist in the training sample due to random variation.\n\nTo illustrate this, we generate a simulation with 100 features `x1`,...,`x100` and one outcome `y`, all of which are independent normal variables. We know from the beginning that `x*` should be useless predictors: they contain no information about `y`.\n\nWe first load `tidyverse`\n\n::: {.cell}\n\n:::\n\nand write a function to generate the data\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data <- function(sample_size = 1000, num_features = 100) {\n  # Predictors are independent Normal\n  X <- replicate(num_features, rnorm(sample_size))\n  colnames(X) <- paste0(\"x\",1:num_features)\n  \n  as_tibble(X) |>\n    # Outcome is an independent Normal\n    mutate(y = rnorm(n()))\n}\n```\n:::\n\nbefore applying that function to generate one sample.\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- generate_data(sample_size = 1000, num_features = 100)\n```\n:::\n\n\n## In-sample prediction\n\nWe then estimate a linear regression model, where `.` includes all the `x1`,...,`x100` features other than `y` as predictors.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(y ~ ., data = data)\n```\n:::\n\nand a benchmark of no model, which only includes an intercept.\n\n::: {.cell}\n\n```{.r .cell-code}\nno_model <- lm(y ~ 1, data = data)\n```\n:::\n\n\nWe know from the simulation that the model is useless: the `x`-variables contain no information about `y`. But if we make predictions in-sample, we will see that the mean squared error of the model is surprisingly lower (better) than no model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata |>\n  mutate(\n    predicted_model = predict(model),\n    predicted_no_model = predict(no_model),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |>\n  select(starts_with(\"squared\")) |>\n  summarize_all(.funs = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                <dbl>                  <dbl>\n1               0.902                   1.00\n```\n:::\n:::\n\n\n## Out-of-sample prediction\n\nThe problem is that the model fit to the noise in the data. We can see this with the out-of-sample performance assessment. First, we generate a new dataset of out-of-sample data.\n\n::: {.cell}\n\n```{.r .cell-code}\nout_of_sample <- generate_data()\n```\n:::\n\n\nThen, we use the model learned in `data` and predict in `out_of_sample`. By this evaluation, predictions are now worse (higher mean squared error) than no model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_of_sample |>\n  mutate(\n    predicted_model = predict(model, newdata = out_of_sample),\n    predicted_no_model = predict(no_model, newdata = out_of_sample),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |>\n  select(starts_with(\"squared\")) |>\n  summarize_all(.funs = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                <dbl>                  <dbl>\n1                1.05                  0.981\n```\n:::\n:::\n\n\n## Split-sample prediction\n\nIn practice, we often do not have a second sample. We can therefore mimic the out-of-sample task by a sample split, which we can create using the `initial_split` function in the `rsample` package,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\nsplit <- initial_split(data, prop = .5)\n```\n:::\n\n\nwhich randomly assigns the data into training and testing sets of equal size. We can extract those data by typing `training(split)` and `testing(split)`.\n\nThe strategy is to learn on `training(split)`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(y ~ ., data = training(split))\nno_model <- lm(y ~ 1, data = training(split))\n```\n:::\n\n\nand evaluate performance on `testing(split)`.\n \n\n::: {.cell}\n\n```{.r .cell-code}\ntesting(split) |>\n  mutate(\n    predicted_model = predict(model, newdata = testing(split)),\n    predicted_no_model = predict(no_model, newdata = testing(split)),\n    squared_error_model = (y - predicted_model) ^ 2,\n    squared_error_no_model = (y - predicted_no_model) ^ 2,\n  ) |>\n  select(starts_with(\"squared\")) |>\n  summarize_all(.funs = mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  squared_error_model squared_error_no_model\n                <dbl>                  <dbl>\n1                1.29                   1.01\n```\n:::\n:::\n\n\nJust like the out-of-sample prediction, this shows that the model is worse than no model at all. By sample splitting, we can learn this even when we have only one sample.\n\nAn important caveat is that sample splitting has a cost: the number of cases available for training is smaller once we split the sample. This can mean that the sample-split predictions will have worse performance than predictions trained on the full sample and evaluated out-of-sample.\n\n## Repeating this many times\n\nIf we repeat the above many times, we can see the distribution of these performance evaluation strategies across repeated samples.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output-display}\n![](sample_splitting_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nBy the gold standard of out-of-sample prediction, no model is better than a model. In-sample prediction yields the misleading appearance that a model is better than no model. Split-sample prediction successfully mimics the out-of-sample behavior when only one sample is available.\n\n## Closing thoughts\n\nSample splitting is an art as much as a science. In particular applications, the gain from sample splitting is not always clear and must be balanced against the reduction in cases available for training. It is important to remember that out-of-sample prediction remains the gold standard, and sample splitting is one way to approximate that when only one sample is available.\n\n",
    "supporting": [
      "sample_splitting_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}