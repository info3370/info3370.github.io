{
  "hash": "8372b9a481ead387b989e86407cd5e5d",
  "result": {
    "markdown": "---\ntitle: \"Statistical Learning\"\n---\n\n\n[Slides](../slides/lec4/lec4.pdf)\n\n> The reading with this class is [Berk 2020 Ch 1](https://link.springer.com/book/10.1007/978-3-030-40189-4) p. 1--5, stopping at paragraph ending \"...is nonlinear.\" Then p. 14--17 \"Model misspecification...\" through \"...will always be in play.\"\n\nStatistical learning is a term that captures a broad set of ideas in statistics and machine learning. This page focuses on one sense of statistical learning: using data on a sample to learn a subgroup mean in the population.\n\n::: columns\n\n::: {.column width=\"30%\"}\n\n{{< video https://www.youtube.com/embed/srf9eZ5lq68 >}}\n\n\n:::\n\n::: {.column width=\"30%\"}\n\n{{< video https://www.youtube.com/embed/3sHWUN-3pZE >}}\n\n\n:::\n\n::: {.column width=\"30%\"}\n\n{{< video https://www.youtube.com/embed/Pk6uNM74cKE >}}\n\n\n:::\n\n:::\n\nAs an example, we continue to use the data on baseball salaries, with a small twist. The file [`baseball_with_record.csv`](../data/baseball_with_record.csv) contains the following variables\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- read_csv(\"https://info3370.github.io/data/baseball_with_record.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n- `player` is the player name\n- `team` is the team name\n- `salary` is the 2023 salary\n- `record` is the 2022 proportion of games won by that team\n- `target_subgroup` is coded `TRUE` for the L.A. Dodgers and `FALSE` for all others\n\n\n::: {.cell}\n\n:::\n\n\nOur goal: using a sample, estimate the mean salary of all Dodger players in 2023. Because we have the population, we know the true mean is $6.23m.\n\nA sparse sample will hinder our ability to accomplish the goal. We will work with samples containing many MLB players, but only a few Dodgers. We will use statistical learning strategies to pool information from those other teams' players to help us make a better estimate of the Dodger mean salary.\n\nOur predictor will be the `record` from the previous year. We assume that teams with similar win percentages in 2022 might have similar salaries in 2023.\n\n## Prepare our data environment\n\nFor illustration, draw a sample of 5 players per team\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample <- population |>\n  group_by(team) |>\n  sample_n(5) |>\n  ungroup()\n```\n:::\n\n\nConstruct a tibble with the observation to be predicted: the Dodgers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- population |>\n  filter(target_subgroup) |>\n  distinct(team, record)\n```\n:::\n\n\n## Ordinary least squares\n\nWe could model salary next year as a linear function of team record by Ordinary Least Squares. In math, OLS produces a prediction\n$$\\hat{Y}_i = \\hat\\alpha + \\hat\\beta X_i$$ \nwith $\\hat\\alpha$ and $\\hat\\beta$ chosen to minimize the sum of squared errors, $\\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2$. Visually, it minimizes all the line segments below.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](statistical_learning_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nHere is how to estimate an OLS model using R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(salary ~ record, data = sample)\n```\n:::\n\n\nThen we could predict the mean salary for the Dodgers.\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict |>\n  mutate(predicted = predict(model, newdata = to_predict))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  team         record predicted\n  <chr>         <dbl>     <dbl>\n1 L.A. Dodgers  0.685  6668053.\n```\n:::\n:::\n\n\nOur model-based estimate compares to the true population mean of $6.23m.\n\n## Penalized regression\n\nPenalized regression is just like OLS, except that it prefers coefficient estimates that are closer to 0. This can reduce sampling variability. One penalized regression is ridge regression, which penalizes the sum of squared coefficients. In our example, it estimates the parameters to minimize\n\n$$\\underbrace{\\sum_{i=1}^n \\left(Y_i - \\hat{Y}_i\\right)^2}_\\text{Squared Error} + \\underbrace{\\lambda\\beta^2}_\\text{Penalty}$$\n\nwhere the positive scalar penalty $\\lambda$ encodes our preference for coefficients to be near zero. Otherwise, penalized regression is just like OLS!\n\nThe `gam()` function in the `mgcv` package will allow you to fit a ridge regression as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- gam(\n  salary ~ s(record, bs = \"re\"),\n  data = sample\n)\n```\n:::\n\n\nPredict the Dodger mean salary just as before,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict |>\n  mutate(predicted = predict(model, newdata = to_predict))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  team         record predicted\n  <chr>         <dbl> <dbl[1d]>\n1 L.A. Dodgers  0.685  6252449.\n```\n:::\n:::\n\n\n## Splines\n\nWe may want to allow a nonlinear relationship between the predictor and the outcome. One way to do that is with splines, which estimate part of the model locally within regions of the predictor space separated by **knots**. The code below uses a linear spline with knots at 0.4 and 0.6.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\nmodel <- lm(\n  salary ~ bs(record, degree = 1, knots = c(.4,.6)),\n  data = sample\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](statistical_learning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nWe can predict the Dodger mean salary just as before!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict |>\n  mutate(predicted = predict(model, newdata = to_predict))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  team         record predicted\n  <chr>         <dbl>     <dbl>\n1 L.A. Dodgers  0.685  3269869.\n```\n:::\n:::\n\n\n## Trees\n\nPerhaps our response surface is bumpy, and poorly approximated by a smooth function. Decision trees search the predictor space for discrete places where the outcome changes, and assume that the response is flat within those regions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nmodel <- rpart(salary ~ record, data = sample)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](statistical_learning_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nPredict as in the other strategies.\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict |>\n  mutate(predicted = predict(model, newdata = to_predict))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  team         record predicted\n  <chr>         <dbl>     <dbl>\n1 L.A. Dodgers  0.685  4253845.\n```\n:::\n:::\n\n\n## Conclusion\n\nStatistical learning in this framing is all about\n\n- we have a subgroup with few sampled units (the Dodgers)\n- we want to use other units to help us learn\n- our goal is to predict the population mean in the subgroup\n",
    "supporting": [
      "statistical_learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}