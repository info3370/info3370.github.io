---
layout: page
nav_order: 4
title: Sample splitting
description: Sample splitting
grand_parent: Topics
parent: Opportunity
---

# Train-test split: Evaluate out-of-sample prediction

Prediction is most interesting for cases we have not yet observed. A
goal of supervised machine learning is to

- take the cases we’ve observed
- learn a prediction function
- predict outcomes for new cases from the same population

Which model will do this the best? Often the answer to that question is
not clear a priori. When selecting a model, we might follow a key tool
of machine learning: mimic the prediction task with a train-test split

- take the cases we’ve observed
- randomly pick half for training and half for testing
- learn a prediction function in the training cases
- predict for the test cases
- measure prediction error in test cases

Then you pick the one that does the best! No matter how complex the
predictive algorithms you want to consider, a train-test split can help
you select the best among them.

The rest of this page

- evaluates one candidate learner
- evalutes several candidate learners

To get started, load the data that we are using in this module. See the
previous pages for data access.

``` r
library(tidyverse)
learning <- read_csv("learning.csv")
holdout_public <- read_csv("holdout_public.csv")
```

# Basics: Evaluate one candidate learner

## Create a train-test split

We observe outcomes in the `learning` data, so we want to split these
randomly into a `train` set and a `test` set. One way to do this is with
the `rsample` package.

``` r
library(rsample)
learning_split <- learning %>%
  initial_split(prop = 0.5)
train <- training(learning_split)
test <- testing(learning_split)
```

## Learn a prediction function on the train set

This could be any function you want. We will illustrate with OLS.

``` r
fit <- lm(g3_log_income ~ g2_log_income + g1_log_income,
          data = train)
```

## Evaluate predictive performance on the test set

``` r
test %>%
  # Make predictions from the model
  mutate(predicted = predict(fit, newdata = test)) %>%
  # Calculate prediction errors
  mutate(error = g3_log_income - predicted,
         squared_error = error ^ 2) %>%
  # Calculate mean squared error
  summarize(mse = mean(squared_error))
```

    ## # A tibble: 1 × 1
    ##     mse
    ##   <dbl>
    ## 1 0.473

# Advanced: Evaluate multiple models

A train-test split is especially helpful when we have **multiple**
candidate learners. Our goal here is to choose the best. To do so, it
may help to wrap the code above within a function that takes a `formula`
and returns an estimate `mse` of mean squared out-of-sample prediction
error.

``` r
evaluate_model <- function(formula, train_data = train, test_data = test) {
  fit <- lm(formula,
            data = train_data)
  result <- test_data %>%
    # Make predictions from the model
    mutate(predicted = predict(fit, newdata = test)) %>%
    # Calculate prediction errors
    mutate(error = g3_log_income - predicted) %>%
    # Calculate mean squared error
    summarize(mse = mean(error ^ 2))
  return(result$mse)
}
```

We can now define several candidate models

``` r
model_1 <- formula(g3_log_income ~ g2_log_income + g1_log_income)
model_2 <- formula(g3_log_income ~ g2_log_income + g1_log_income + g3_educ)
model_3 <- formula(g3_log_income ~ (g2_log_income + g1_log_income)*g3_educ)
```

and evaluate performance of each model

``` r
performance <- c(evaluate_model(model_1),
                 evaluate_model(model_2),
                 evaluate_model(model_3))
names(performance) <- c("Model 1", "Model 2", "Model 3")
print(performance)
```

    ##   Model 1   Model 2   Model 3 
    ## 0.4733473 0.4466595 0.4580666

In this case, Model 3 is the best!

## A tidyverse way of doing this

If you want very clean code, this can also be done via `tidyverse` using
[nested
tibbles](https://r4ds.had.co.nz/many-models.html?q=nest#many-models),
which can hold arbitrary objects like model formulas within an object
like a data frame. This is an advanced topic; it is included here only
to illustrate that tidy code is possible.

``` r
tibble(
  # Define 3 candidate formulas
  candidate_formula = list(formula(g3_log_income ~ g2_log_income + g1_log_income),
                           formula(g3_log_income ~ g2_log_income + g1_log_income + g3_educ),
                           formula(g3_log_income ~ (g2_log_income + g1_log_income)*g3_educ))
) %>%
  # Calculate MSE for each formula
  # To do so, map the formula to the function evaluate_model
  mutate(mse = map_vec(candidate_formula, evaluate_model)) %>%
  # Create a variable with model names
  mutate(model_name = c("Model 1", "Model 2", "Model 3")) %>%
  # Create a ggplot
  ggplot(aes(x = model_name, y = mse)) +
  geom_point() +
  xlab("Candidate Prediction Algorithm") +
  ylab("Test Set Mean Squared Error")
```

![](train_test_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->
