---
title: "Statistical Learning"
---

[Slides](../slides/lec4/lec4.pdf)

> The reading with this class is [Berk 2020 Ch 1](https://link.springer.com/book/10.1007/978-3-030-40189-4) p. 1--5, stopping at paragraph ending "...is nonlinear." Then p. 14--17 "Model misspecification..." through "...will always be in play."

Statistical learning is a term that captures a broad set of ideas in statistics and machine learning. This page focuses on one sense of statistical learning: using data on a sample to learn a subgroup mean in the population.

::: columns

::: {.column width="30%"}
{{< video https://www.youtube.com/embed/srf9eZ5lq68 >}}
:::

::: {.column width="30%"}
{{< video https://www.youtube.com/embed/3sHWUN-3pZE >}}
:::

::: {.column width="30%"}
{{< video https://www.youtube.com/embed/Pk6uNM74cKE >}}
:::

:::

As an example, we continue to use the data on baseball salaries, with a small twist. The file [`baseball_with_record.csv`](../data/baseball_with_record.csv) contains the following variables

```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(scales)
```

```{r, eval = F}
population <- read_csv("https://info3370.github.io/data/baseball_with_record.csv")
```
```{r, message = F, warning = F, echo = F}
population <- read_csv("../data/baseball_with_record.csv")
set.seed(14853)
```

- `player` is the player name
- `team` is the team name
- `salary` is the 2023 salary
- `record` is the 2022 proportion of games won by that team
- `target_subgroup` is coded `TRUE` for the L.A. Dodgers and `FALSE` for all others

```{r, echo = F}
truth <- population |> filter(target_subgroup) |> summarize(salary = mean(salary)) |> mutate(salary = label_dollar(scale = 1e-6, suffix = "m")(salary)) |> pull(salary)
```

Our goal: using a sample, estimate the mean salary of all Dodger players in 2023. Because we have the population, we know the true mean is `r truth`.

A sparse sample will hinder our ability to accomplish the goal. We will work with samples containing many MLB players, but only a few Dodgers. We will use statistical learning strategies to pool information from those other teams' players to help us make a better estimate of the Dodger mean salary.

Our predictor will be the `record` from the previous year. We assume that teams with similar win percentages in 2022 might have similar salaries in 2023.

## Prepare our data environment

For illustration, draw a sample of 5 players per team

```{r}
sample <- population |>
  group_by(team) |>
  sample_n(5) |>
  ungroup()
```

Construct a tibble with the observation to be predicted: the Dodgers.

```{r}
to_predict <- population |>
  filter(target_subgroup) |>
  distinct(team, record)
```

## Ordinary least squares

We could model salary next year as a linear function of team record by Ordinary Least Squares. In math, OLS produces a prediction
$$\hat{Y}_i = \hat\alpha + \hat\beta X_i$$ 
with $\hat\alpha$ and $\hat\beta$ chosen to minimize the sum of squared errors, $\sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2$. Visually, it minimizes all the line segments below.

```{r, echo = F, fig.height = 2}
fit <- lm(salary ~ record, data = sample)
sample |>
  mutate(fitted = predict(fit)) |>
  ggplot(aes(x = record, y = salary)) +
  geom_segment(aes(xend = record, yend = fitted), color = "gray") +
  geom_point(data = sample, size = .5) +
  geom_line(aes(y = fitted)) +
  scale_y_continuous(
    name = "Mean Salary on Team",
    labels = label_dollar(scale = 1e-6, suffix = "m")
  ) +
  scale_x_continuous(
    name = "Past Team Win-Loss Record"
  )
```

Here is how to estimate an OLS model using R.

```{r}
model <- lm(salary ~ record, data = sample)
```

Then we could predict the mean salary for the Dodgers.
```{r}
to_predict |>
  mutate(predicted = predict(model, newdata = to_predict))
```

Our model-based estimate compares to the true population mean of `r truth`.

## Penalized regression

Penalized regression is just like OLS, except that it prefers coefficient estimates that are closer to 0. This can reduce sampling variability. One penalized regression is ridge regression, which penalizes the sum of squared coefficients. In our example, it estimates the parameters to minimize

$$\underbrace{\sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2}_\text{Squared Error} + \underbrace{\lambda\beta^2}_\text{Penalty}$$

where the positive scalar penalty $\lambda$ encodes our preference for coefficients to be near zero. Otherwise, penalized regression is just like OLS!

The `gam()` function in the `mgcv` package will allow you to fit a ridge regression as follows.

```{r, message = F, warning = F}
library(mgcv)
```

```{r}
model <- gam(
  salary ~ s(record, bs = "re"),
  data = sample
)
```

Predict the Dodger mean salary just as before,

```{r}
to_predict |>
  mutate(predicted = predict(model, newdata = to_predict))
```

## Splines

We may want to allow a nonlinear relationship between the predictor and the outcome. One way to do that is with splines, which estimate part of the model locally within regions of the predictor space separated by **knots**. The code below uses a linear spline with knots at 0.4 and 0.6.

```{r}
library(splines)
model <- lm(
  salary ~ bs(record, degree = 1, knots = c(.4,.6)),
  data = sample
)
```

```{r, echo = F, fig.height = 2}
to_predict_all <- population |>
  distinct(team, record, target_subgroup)
to_predict_all |>
  mutate(fitted = predict(model, newdata = to_predict_all)) |>
  ggplot(aes(x = record, y = fitted)) +
  geom_line() +
  geom_point(aes(color = target_subgroup), size = 2) +
  scale_color_manual(values = c("black","dodgerblue")) +
  theme_classic() +
  theme(legend.position = "none") +
  scale_y_continuous(
    name = "Team Mean Salary",
    labels = label_dollar(scale = 1e-6, suffix = "m"),
    limits = c(0,10e6)
  ) +
  scale_x_continuous(name = "Past Team Win-Loss Record")
```

We can predict the Dodger mean salary just as before!

```{r}
to_predict |>
  mutate(predicted = predict(model, newdata = to_predict))
```

## Trees

Perhaps our response surface is bumpy, and poorly approximated by a smooth function. Decision trees search the predictor space for discrete places where the outcome changes, and assume that the response is flat within those regions.

```{r}
library(rpart)
model <- rpart(salary ~ record, data = sample)
```

```{r, echo = F, fig.height = 2}
to_predict_all |>
  mutate(fitted = predict(model, newdata = to_predict_all)) |>
  ggplot(aes(x = record, y = fitted)) +
  geom_step() +
  geom_point(aes(color = target_subgroup), size = 2) +
  scale_color_manual(values = c("black","dodgerblue")) +
  theme_classic() +
  theme(legend.position = "none") +
  scale_y_continuous(
    name = "Team Mean Salary",
    labels = label_dollar(scale = 1e-6, suffix = "m"),
    limits = c(0,10e6)
  ) +
  scale_x_continuous(name = "Past Team Win-Loss Record")
```

Predict as in the other strategies.
```{r}
to_predict |>
  mutate(predicted = predict(model, newdata = to_predict))
```

## Conclusion

Statistical learning in this framing is all about

- we have a subgroup with few sampled units (the Dodgers)
- we want to use other units to help us learn
- our goal is to predict the population mean in the subgroup
