---
title: "Population Sampling"
---

```{r, echo = F, message = F, warning = F}
library(tidyverse)
library(scales)
```

Claims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.

This page covers two lecture (1/30 and 2/6).

- Lecture on 1/30 [[slides]](../slides/lec2/lec2.pdf) covers [full count enumeration](#full-count-enumeration) through the [Current Population Survey](#sec-cps). After lecture, you should read about [probability sampling](https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch13/prob/5214899-eng.htm) and register for an account at [cps.ipums.org](https://cps.ipums.org/cps/).
- Lecture on 2/6 [[slides]](../slides/lec3/lec3.pdf) uses salaries of [Major League Baseball players](#sec-baseball) to carry out three sampling strategies and explore their performance. After class, you should read [Groves 2011](https://academic.oup.com/poq/article/75/5/861/1831518) on the past and future of sampling

::: {.callout-tip}
### Heads-up!
You will want your computer in lecture on 2/6.
:::
    
## Full count enumeration

What proportion of our class prefers to sit in the front of the room?

We answered this question in class using **full count enumeration**: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.

## Simple random sample

We carried out a **simple random sample**^[Technically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.] in class.

- everyone generated a random number between 0 and 1
- those with values less than 0.1 were sampled
- our sample estimate was the proportion of those sampled to prefer the front of the room

In a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a **probability sample**.

{{< video https://www.youtube.com/embed/IDGKgpiM218 >}}

## Unequal probability sample

Suppose we want to make subgroup estimates:

- what proportion prefer the front, among those sitting in the first 3 rows?
- what proportion prefer the front, among those sitting in the back 17 rows?

In a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:

- those in rows 1--3 are selected with probability 0.5
- those in rows 4--20 are selected with probability 0.1

Our unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.

{{< video https://www.youtube.com/embed/4r_85uHrNY0 >}}

Having drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.

Because the sampling probabilities are known, we can correct for this by applying **sampling weights**, which for each person equals the inverse of the known probability of inclusion for that person.

For those in rows 1--3,

- we sampled with probability 50\%
- on average 1 in every 2 people is sampled
- each person in the sample represents 2 people in the population
- $w_i = \frac{1}{\text{P}(\text{Sampled}_i)} = \frac{1}{.5} = 2$

For those in rows 4--20,

- we sampled with probability 10\%
- on average 1 in every 10 people is sampled
- each person in the sample represents 10 people in the population
- $w_i = \frac{1}{\text{P}(\text{Sampled}_i)} = \frac{1}{.1} = 10$

To estimate the population mean, we can use the weighted sample mean,

$$\frac{\sum_i y_iw_i}{\sum_i w_i}$$

{{< video https://www.youtube.com/embed/nczwdCiTmMk >}}

## Stratified random sample

We could also draw a **stratified random sample** by first partitioning the population into subgroups (called **strata**) and then drawing samples within each subgroup. For instance,

- sample 10 of the 20 people in rows 1--3
- sample 10 of the 130 people in rows 4--17

In simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.

::: {.callout-note}
In our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.
:::

## A real case: The Current Population Survey {#sec-cps}

Every month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.

We will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via [IPUMS-CPS](https://cps.ipums.org/cps/).

{{< video https://www.youtube.com/embed/1VxcmPOWwG0 >}}

## Example: Baseball players {#sec-baseball}

As one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by [USA Today](https://databases.usatoday.com/major-league-baseball-salaries-2023/) and are available in [baseball.csv](../data/baseball.csv).

```{r, eval = F}
baseball <- read_csv("https://info3370.github.io/data/baseball.csv")
```

```{r, echo = F, message = F, warning = F}
baseball <- read_csv("../data/baseball.csv") |>
  arrange(-salary) |>
  select(player, team, position, salary) |>
  print(n = 5)
baseball_highest <- baseball |>
  filter(salary == max(salary)) |>
  mutate(player = paste(
    str_replace_all(player,".*, ",""),
    str_replace_all(player,",.*","")
  ))
top_50pct_pay_portion <- baseball |>
  arrange(-salary) |>
  mutate(
    cdf = 1:n() / n(),
    top = cdf <= .5
  ) |>
  group_by(top) |>
  summarize(salary = sum(salary)) |>
  mutate(percent_of_total = salary / sum(salary)) |>
  filter(top) |>
  pull(percent_of_total) |>
  label_percent()()
```

Salaries are high, and income inequality is also high among baseball players

- `r label_percent()(mean(baseball$salary == min(baseball$salary)))` were paid the league minimum of \$720,000
- `r label_percent()(mean(baseball$salary < 2e6))` were paid less than \$2,000,000
- the highest-paid players---`r paste(baseball_highest$player, collapse = " and ")`---each earned `r label_dollar()(baseball_highest$salary[1])`
- the highest-paid half of players take home `r top_50pct_pay_portion` of the total pay

```{r, echo = F, fig.height = 2}
baseball |>
  ggplot(aes(x = salary)) +
  geom_histogram(bins = 50) +
  scale_x_continuous(
    name = "Annual Salary",
    labels = label_dollar()
  ) +
  ylab("Count") +
  ggtitle(paste("Salary of all ",nrow(baseball),"MLB players on Opening Day 2023"))
```

Pay also varies widely across teams!

```{r, echo = F, fig.height = 3, warning = F}
baseball |>
  group_by(team) |>
  summarize(salary_mean = mean(salary), salary_sd = sd(salary)) |>
  ggplot(aes(x = salary_mean, y = salary_sd)) +
  geom_point() +
  geom_text(aes(
    label = case_when(salary_mean %in% range(salary_mean) ~ team), 
    hjust = case_when(
      team == "Oakland" ~ -.2,
      team == "N.Y. Mets" ~ 1.2
    )
  )) +
  scale_x_continuous(name = "Average Salary", labels = label_dollar()) +
  scale_y_continuous(name = "Standard Deviation of Salary", labels = label_dollar()) +
  theme(plot.margin = unit(c(5.5,20,5.5,5.5),"pt"))
```

### Conceptualize the sampling strategy

Suppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?

Before reading on, think through three questions:

1) What would it mean to use each of these strategies?

- a simple random sample of 60 players
- a sample stratified by the 30 MLB teams
- a sample clustered by the 30 MLB teams

2) Which strategies have advantages in terms of

- being least expensive?
- having the best statistical properties?

3) Given that you already have the population, how would you write some R code to carry out the sampling strategies? You might use `sample_n()` and possibly `group_by()`.

### Sampling strategies in code

In a **simple random sample**, we draw 60 players from the entire league. Each player's probability of sample inclusion is $\frac{60}{n}$ where $n$ is the number of players in the league (`r nrow(baseball)`).

```{r}
simple_sample <- function(population) {
  population |>
    # Define sampling probability and weight
    mutate(
      p_sampled = 60 / n(),
      sampling_weight = 1 / p_sampled
    ) |>
    # Sample 60 players
    sample_n(size = 60)
}
```

To use this function, we give it the `baseball` data as the population and it returns a tibble containing a sample of 60 players.

```{r}
simple_sample(population = baseball)
```

In a **stratified random sample** by team, we sample 2 players on each of 30 teams.

Each player's probability of sample inclusion is $\frac{2}{n}$ where $n$ is the number on that player's team (which ranges from `r baseball |> group_by(team) |> count() |> pull(n) |> range() |> paste(collapse = " to ")`).

```{r}
stratified_sample <- function(population) {
  population |>
    # Draw sample within each team
    group_by(team) |>
    # Define sampling probability and weight
    mutate(
      p_sampled = 2 / n(),
      sampling_weight = 1 / p_sampled
    ) |>
    # Within each team, sample 2 players
    sample_n(size = 2) |>
    ungroup()
}
```

In a sample **clustered by team**, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams.

Each player's probability of sample inclusion is P(Team Chosen) $\times$ P(Chosen Within Team) = $\frac{3}{30}\times\frac{20}{n}$ where $n$ is the number on that player's team (which ranges from `r baseball |> group_by(team) |> count() |> pull(n) |> range() |> paste(collapse = " to ")`).

```{r}
clustered_sample <- function(population) {
  
  # First, sample 3 teams
  sampled_teams <- population |>
    # Make one row per team
    distinct(team) |>
    # Sample 3 teams
    sample_n(3) |>
    # Store those 3 team names in a vector
    pull()
  
  # Then load data on those teams and sample 20 per team
  population |>
    filter(team %in% sampled_teams) |>
    # Define sampling probability and weight
    group_by(team) |>
    mutate(
      p_sampled = (3 / 30) * (20 / n()),
      sampling_weight = 1 / p_sampled
    ) |>
    # Sample 20 players
    sample_n(20) |>
    ungroup()
}
```

### Weighted mean estimator

Given a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function

- we hand our sample to the function
- we get a numeric estimate back

```{r}
estimator <- function(sample) {
  sample |>
    summarize(estimate = weighted.mean(
      x = salary, 
      w = sampling_weight
    )) |>
    pull(estimate)
}
```

Here is what it looks like to use the estimator.

```{r}
sample_example <- simple_sample(population = baseball)
estimator(sample = sample_example)
```

Try it for yourself! The true mean salary in the league is `r label_dollar()(mean(baseball$salary))`. How close do you come when you apply the estimator to a sample drawn by each strategy?

### Evaluating performance: Many samples

We might like to know something about performance across many repeated samples. The `replicate` function will carry out a set of code many times.

```{r}
sample_estimates <- replicate(
  n = 1000,
  expr = {
    a_sample <- simple_sample(population = baseball)
    estimator(sample = a_sample)
  }
)
```

Simulate many samples. Which one is the best? Strategy A, B, or C?

{{< video https://www.youtube.com/embed/K38fqGB60no >}}

### The danger of one sample

In actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.

Does salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the `position` variable takes the value `LHP` or `RHP`).

```{r}
pitchers <- baseball |>
  filter(position == "LHP" | position == "RHP")
```

To illustrate what can happen with a sample, we now draw a sample. Let's first set our computer's random number seed so we get the same sample each time.

```{r}
set.seed(1599)
```

Then draw a sample of 40 pitchers 

```{r}
pitchers_sample <- pitchers |>
  sample_n(size = 40)
```

and examine the mean difference in salary.

```{r}
pitchers_sample |>
  group_by(position) |>
  summarize(salary_mean = mean(salary))
```

The left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren't many available because so few people are left-handed!

What happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.

```{r, echo = F, message = F, warning = F}
library(foreach)
test_difference <- function(data) {
  if (data |> nrow() <= 2) {
    stop("Error: Expects a data frame with more than 2 rows. There should be one row per player")
  }
  if (!any(data |> colnames() == "salary")) {
    stop("Error: Expects salary to be a column of the data")
  }
  if (!any(data |> colnames() == "position")) {
    stop("Error: Expects salary to be a column of the data")
  }
  if (data |> distinct(position) |> nrow() != 2) {
    stop("Error: Expects a data frame with 2 distinct positions")
  }
  
  # Produce estimate in each subgroup
  subgroup_estimates <- data |>
    group_by(position) |>
    summarize(estimate = mean(salary)) |>
    pivot_wider(names_from = "position", values_from = "estimate")
  
  # Produce difference estimate with significance test
  test_of_difference <- data |>
    arrange(position) |>
    group_by(position) |>
    summarize(estimate = mean(salary),
              estimate_var = var(salary) / n()) |>
    summarize(difference = -diff(estimate),
              standard_error = sqrt(sum(estimate_var))) |>
    mutate(
      ci_min = difference - qnorm(.975) * standard_error,
      ci_max = difference + qnorm(.975) * standard_error,
      p_value = 2 * pnorm(abs(difference) / standard_error, lower.tail = F),
      significant = p_value < .05
    )
  
  # Return a combined data frame
  return(
    subgroup_estimates |>
      bind_cols(test_of_difference)
  )
}

many_sample_estimates <- foreach(
  rep = 1:1000,
  .combine = "rbind"
) %do% {
  pitchers |>
    sample_n(40) |>
    test_difference()
}

pct_significant <- many_sample_estimates |>
  summarize(significant = mean(significant)) |>
  mutate(significant = paste0(round(100*significant),"%")) |>
  pull(significant)

truth <- pitchers |>
  group_by(position) |>
  summarize(estimate = mean(salary)) |>
  pivot_wider(names_from = "position", 
              values_from = "estimate") |>
  mutate(difference = LHP - RHP)

many_sample_estimates |>
  arrange(significant) |>
  ggplot(aes(x = 1, y = difference, color = significant, alpha = significant)) +
  geom_jitter(width = .2, height = 0) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  scale_color_manual(
    name = "Statistically\nSignificant",
    values = c("gray","seagreen4")
  ) +
  scale_alpha_manual(
    name = "Statistically\nSignificant",
    values = c(.5,1)
  ) +
  scale_y_continuous(
    name = "Mean Salary Difference\nLeft - Right Handed Pitchers",
    labels = label_dollar(),
    breaks = seq(-5e6,5e6,2.5e6)
  ) +
  scale_x_continuous(
    limits = c(.7,1.3),
    name = element_blank(),
    breaks = NULL
  ) +
  theme(panel.grid = element_blank(),
        panel.background = element_blank()) +
  ggtitle(
    "Distribution of Sample Estimates",
    subtitle = paste(pct_significant,"of sample estimates are\nstatistically significant")
  )
```

Our original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!

This exercise illustrates what is known as the **replication crisis**: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.

One solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.

{{< video https://www.youtube.com/embed/VN8-54WcJfM >}}

## The future of sample surveys

Sample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:

- some sampling frames, such as landline telephones, have become obsolete
- response rates have been falling for decades
- sample surveys are slower and more expensive than digital data

What is the future for sample surveys? How can they be combined with other data?

We will close with a discussion of these questions, which you can also engage with in the [Groves 2011](https://academic.oup.com/poq/article/75/5/861/1831518) reading that follows this module.

{{< video https://www.youtube.com/embed/k4o0RA78kxA >}}
