[
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team! For office hours, see the Syllabus.\n\n\n\n\n\n\n\n\nIan Lundberg\nilundberg@cornell.edu\n(he / him)\nWorking with data to understand inequality brings me joy and meaning, as I first discovered as a college student years ago. I hope to share that joy with you! Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nFederica Bologna\nfb265@cornell.edu\n(she / her)\nI am passionate about using data to support patient-centered care and study gender inequality in fiction readership. I love binge-watching shows, hiking, and making food with friends. Excited to meet you in class!\n\n\n\n\n\nDaniel Molitor\ndmolitor@infosci.cornell.edu\nMy research interests primarily focus on applying computational methods to important policy questions. Developing software and building tools is also something I love to do! In my free time I enjoy doing pretty much anything that’s active and outdoors and I particularly enjoy playing tennis, kayaking, and fishing.\n\n\n\n\n\n\n\n\nStacy Boey\n(she / her)\nHi everyone! I am an information science major concentrating in data science and UX. Some hobbies of mine include crocheting, thrifting, and cooking. I look forward to a great semester with you guys!\n\n\n\n\n\nChang Chen\nHi everyone! I study biometry and information science, concentrating on data science. I enjoy playing badminton and cooking. I look forward to working with all of you!\n\n\n\n\n\nOlaf de Rohan Willner\nHello everyone! I study Information Science and Government and am fascinated by understanding the world and how policies shape it through data. Outside of class I love watching 1950’s French movies, running, and cooking. Looking forward to meeting you all this semester!\n\n\n\n\n\nAkira Goh\nHi everyone, I am a Dyson and Infoscience double major. I love scubadiving, snowboarding, and working out. Excited to be working with all of you.\n\n\n\n\n\nJonathan Gotian\n(he / him)\nHi all, I am a Dyson major and pursuing an Information Science minor studying business analytics and data science. In my free time, I love playing squash and chess. Excited to work with you all this semester!\n\n\n\n\n\nJoanne/Xinyu Hu\n(she / they)\nHey all! I am an Information Science and Communication double major. Thinking about socioeconomic inequalities is always an important framework for me to see the world, and looking forward to working with you all in the class! Outside schoolwork, I enjoy biking, exploring storytelling, and visiting museums (LA The Broad is my favorite in the US so far but always open to recommendations)\n\n\n\n\n\nLiz Moon\nHi there! I study Information and Computer Science with a focus on Data Science. I am passionate about using data-driven solutions to solve social science challenges. My other hobbies include experimenting with fusion cooking and film analysis. Looking forward to another great semester with everyone!"
  },
  {
    "objectID": "topics/weights.html",
    "href": "topics/weights.html",
    "title": "Weights",
    "section": "",
    "text": "When studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with unequal probabilities. We need sample weights to address this fact.\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size—even tiny states like Wyoming. In order to make those estimates, the CPS oversamples people who live in small states.\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#how-survey-designers-create-weights",
    "href": "topics/weights.html#how-survey-designers-create-weights",
    "title": "Weights",
    "section": "How survey designers create weights",
    "text": "How survey designers create weights\nTo calculate sampling weight on person \\(i\\), those who design survey samples take the ratio \\[\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}\\] You can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1% chance of inclusion, then on average 1 of them will be in the sample. That person represents \\(\\frac{1}{.01}=100\\) people.\n\n\n\n\n\n\nExample redux: California and Wyoming\n\n\n\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#point-estimates",
    "href": "topics/weights.html#point-estimates",
    "title": "Weights",
    "section": "Point estimates",
    "text": "Point estimates\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#standard-errors",
    "href": "topics/weights.html#standard-errors",
    "title": "Weights",
    "section": "Standard errors",
    "text": "Standard errors\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\nUnfortunately, the formula you learned doesn’t work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval1 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#computational-strategy-for-replicate-weights",
    "href": "topics/weights.html#computational-strategy-for-replicate-weights",
    "title": "Weights",
    "section": "Computational strategy for replicate weights",
    "text": "Computational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#application-in-the-cps",
    "href": "topics/weights.html#application-in-the-cps",
    "title": "Weights",
    "section": "Application in the CPS",
    "text": "Application in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#footnotes",
    "href": "topics/weights.html#footnotes",
    "title": "Weights",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/sampling.html",
    "href": "topics/sampling.html",
    "title": "Population Sampling",
    "section": "",
    "text": "Claims about inequality are often claims about a population. Our data are typically only a sample! This module addresses the link between samples and populations.\nThis page covers two lecture (1/30 and 2/6).",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#full-count-enumeration",
    "href": "topics/sampling.html#full-count-enumeration",
    "title": "Population Sampling",
    "section": "Full count enumeration",
    "text": "Full count enumeration\nWhat proportion of our class prefers to sit in the front of the room?\nWe answered this question in class using full count enumeration: list the entire target population and ask them the question. Full count enumeration is ideal because it removes all statistical sources of error. But in settings with a larger target population, the high cost of full count enumeration may be prohibitive.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#simple-random-sample",
    "href": "topics/sampling.html#simple-random-sample",
    "title": "Population Sampling",
    "section": "Simple random sample",
    "text": "Simple random sample\nWe carried out a simple random sample1 in class.\n\neveryone generated a random number between 0 and 1\nthose with values less than 0.1 were sampled\nour sample estimate was the proportion of those sampled to prefer the front of the room\n\nIn a simple random sample, each person in the population is sampled with equal probabilities. Because the probabilities are known, a simple random sample is a probability sample.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#unequal-probability-sample",
    "href": "topics/sampling.html#unequal-probability-sample",
    "title": "Population Sampling",
    "section": "Unequal probability sample",
    "text": "Unequal probability sample\nSuppose we want to make subgroup estimates:\n\nwhat proportion prefer the front, among those sitting in the first 3 rows?\nwhat proportion prefer the front, among those sitting in the back 17 rows?\n\nIn a simple random sample, we might only get a few or even zero people in the first 3 rows! To reduce the chance of this bad sample, we could draw an unequal probability sample:\n\nthose in rows 1–3 are selected with probability 0.5\nthose in rows 4–20 are selected with probability 0.1\n\nOur unequal probability sample will over-represent the first three rows, thus creating a large enough sample in this subgroup to yield precise estimates.\n\nHaving drawn an unequal probability sample, suppose we now want to estimate the class-wide proportion who prefer sitting in the front. We will have a problem: those who prefer the front may be more likely to sit there, and they are also sampled with a higher probability! Sample inclusion is related to the value of our outcome.\nBecause the sampling probabilities are known, we can correct for this by applying sampling weights, which for each person equals the inverse of the known probability of inclusion for that person.\nFor those in rows 1–3,\n\nwe sampled with probability 50%\non average 1 in every 2 people is sampled\neach person in the sample represents 2 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.5} = 2\\)\n\nFor those in rows 4–20,\n\nwe sampled with probability 10%\non average 1 in every 10 people is sampled\neach person in the sample represents 10 people in the population\n\\(w_i = \\frac{1}{\\text{P}(\\text{Sampled}_i)} = \\frac{1}{.1} = 10\\)\n\nTo estimate the population mean, we can use the weighted sample mean,\n\\[\\frac{\\sum_i y_iw_i}{\\sum_i w_i}\\]",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#stratified-random-sample",
    "href": "topics/sampling.html#stratified-random-sample",
    "title": "Population Sampling",
    "section": "Stratified random sample",
    "text": "Stratified random sample\nWe could also draw a stratified random sample by first partitioning the population into subgroups (called strata) and then drawing samples within each subgroup. For instance,\n\nsample 10 of the 20 people in rows 1–3\nsample 10 of the 130 people in rows 4–17\n\nIn simple random or unequal probability sampling, it is always possible that by random chance we sample no one in the front of the room. Stratified random sampling rules this out: we know in advance how our sample will be balanced across the two strata.\n\n\n\n\n\n\nNote\n\n\n\nIn our real-data example at the end of this page, the Current Population Survey is stratified by state so that the Bureau of Labor Statistics knows in advance that they will gather a sufficient sample to estimate unemployment in each state.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-cps",
    "href": "topics/sampling.html#sec-cps",
    "title": "Population Sampling",
    "section": "A real case: The Current Population Survey",
    "text": "A real case: The Current Population Survey\nEvery month, the Bureau of Labor Statistics in collaboration with the U.S. Census Bureau collects data on unemployment in the Current Population Survey (CPS). The CPS is a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\nWe will be using the CPS in discussion. This video introduces the CPS and points you toward where you can access the data via IPUMS-CPS.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#sec-baseball",
    "href": "topics/sampling.html#sec-baseball",
    "title": "Population Sampling",
    "section": "Example: Baseball players",
    "text": "Example: Baseball players\nAs one example where full-count enumeration is possible, we will examine the salaries of all 944 Major League Baseball Players who were on active rosters, injured lists, and restricted lists on Opening Day 2023. These data were compiled by USA Today and are available in baseball.csv.\n\nbaseball &lt;- read_csv(\"https://info3370.github.io/data/baseball.csv\")\n\n\n\n# A tibble: 944 × 4\n  player            team         position   salary\n  &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n1 Scherzer, Max     N.Y. Mets    RHP      43333333\n2 Verlander, Justin N.Y. Mets    RHP      43333333\n3 Judge, Aaron      N.Y. Yankees OF       40000000\n4 Rendon, Anthony   L.A. Angels  3B       38571429\n5 Trout, Mike       L.A. Angels  OF       37116667\n# ℹ 939 more rows\n\n\nSalaries are high, and income inequality is also high among baseball players\n\n4% were paid the league minimum of $720,000\n53% were paid less than $2,000,000\nthe highest-paid players—Max Scherzer and Justin Verlander—each earned $43,333,333\nthe highest-paid half of players take home 92% of the total pay\n\n\n\n\n\n\n\n\n\n\nPay also varies widely across teams!\n\n\n\n\n\n\n\n\n\n\nConceptualize the sampling strategy\nSuppose you did not have the whole population. You still want to learn the population mean salary! How could you learn that in a sample of 60 out of the 944 players?\nBefore reading on, think through three questions:\n\nWhat would it mean to use each of these strategies?\n\n\na simple random sample of 60 players\na sample stratified by the 30 MLB teams\na sample clustered by the 30 MLB teams\n\n\nWhich strategies have advantages in terms of\n\n\nbeing least expensive?\nhaving the best statistical properties?\n\n\nGiven that you already have the population, how would you write some R code to carry out the sampling strategies? You might use sample_n() and possibly group_by().\n\n\n\nSampling strategies in code\nIn a simple random sample, we draw 60 players from the entire league. Each player’s probability of sample inclusion is \\(\\frac{60}{n}\\) where \\(n\\) is the number of players in the league (944).\n\nsimple_sample &lt;- function(population) {\n  population |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 60 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Sample 60 players\n    sample_n(size = 60)\n}\n\nTo use this function, we give it the baseball data as the population and it returns a tibble containing a sample of 60 players.\n\nsimple_sample(population = baseball)\n\n# A tibble: 60 × 6\n   player           team         position   salary p_sampled sampling_weight\n   &lt;chr&gt;            &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Adrianza, Ehire  Atlanta      3B        1000000    0.0636            15.7\n 2 Guthrie, Dalton  Philadelphia OF         722500    0.0636            15.7\n 3 Urena, Jose      Colorado     RHP       3000000    0.0636            15.7\n 4 Ruiz, Keibert    Washington   C         1375000    0.0636            15.7\n 5 Abreu, Jose      Houston      1B       19500000    0.0636            15.7\n 6 Blackburn, Paul* Oakland      RHP       1900000    0.0636            15.7\n 7 Donovan, Brendan St. Louis    3B         728600    0.0636            15.7\n 8 Coleman, Dylan   Kansas City  RHP        731750    0.0636            15.7\n 9 Marquez, German  Colorado     RHP      15300000    0.0636            15.7\n10 Varsho, Daulton  Toronto      OF        3050000    0.0636            15.7\n# ℹ 50 more rows\n\n\nIn a stratified random sample by team, we sample 2 players on each of 30 teams.\nEach player’s probability of sample inclusion is \\(\\frac{2}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\nstratified_sample &lt;- function(population) {\n  population |&gt;\n    # Draw sample within each team\n    group_by(team) |&gt;\n    # Define sampling probability and weight\n    mutate(\n      p_sampled = 2 / n(),\n      sampling_weight = 1 / p_sampled\n    ) |&gt;\n    # Within each team, sample 2 players\n    sample_n(size = 2)\n}\n\nIn a sample clustered by team, we might first sample 3 teams and then sample 20 players on each sampled team. A clustered sample is often less costly, for example because you would only need to call up the front office of 3 teams instead of 30 teams.\nEach player’s probability of sample inclusion is P(Team Chosen) \\(\\times\\) P(Chosen Within Team) = \\(\\frac{3}{30}\\times\\frac{20}{n}\\) where \\(n\\) is the number on that player’s team (which ranges from 28 to 35).\n\n# Strategy C\nclustered_sample &lt;- function(population) {\n  \n  # First, sample 3 teams\n  sampled_teams &lt;- population |&gt;\n    # Make one row per team\n    distinct(team) |&gt;\n    # Sample 3 teams\n    sample_n(3) |&gt;\n    # Store those 3 team names in a vector\n    pull()\n  \n  # Then load data on those teams and sample 20 per team\n  population |&gt;\n    filter(team %in% sampled_teams) |&gt;\n    # Define sampling probability and weight\n    group_by(team) |&gt;\n    mutate(\n      p_sampled = (3 / 30) * (20 / n()),\n      sampling_weight = 1 / p_sampled\n    )\n    # Sample 20 players\n    sample_n(20) |&gt;\n    ungroup()\n}\n\n\n\nWeighted mean estimator\nGiven a sample, how do we estimate the population mean? The weighted mean estimator can also be placed in a function\n\nwe hand our sample to the function\nwe get a numeric estimate back\n\n\nestimator &lt;- function(sample) {\n  sample |&gt;\n    summarize(estimate = weighted.mean(\n      x = salary, \n      w = sampling_weight\n    )) |&gt;\n    pull(estimate)\n}\n\nHere is what it looks like to use the estimator.\n\nsample_example &lt;- simple_sample(population = baseball)\nestimator(sample = sample_example)\n\n[1] 4331742\n\n\nTry it for yourself! The true mean salary in the league is $4,965,481. How close do you come when you apply the estimator to a sample drawn by each strategy?\n\n\nEvaluating performance: Many samples\nWe might like to know something about performance across many repeated samples. The replicate function will carry out a set of code many times.\n\nsample_estimates &lt;- replicate(\n  n = 1000,\n  expr = {\n    a_sample &lt;- simple_sample(population = baseball)\n    estimator(sample = a_sample)\n  }\n)\n\nSimulate many samples. Which one is the best? Strategy A, B, or C?\n\n\n\nThe danger of one sample\nIn actual science, we typically have only one sample. Any estimate we produce from that sample involves some signal about the population quantities, and also some noise. Herein is the danger: researchers are very good at telling stories about why their sample evidence tells something about the population, even when it may be random noise. We illustrate this with an example.\nDoes salary differ between left- and right-handed pitchers? To address this question, I create a tibble with only the pitchers(those for whom the position variable takes the value LHP or RHP).\n\npitchers &lt;- baseball |&gt;\n  filter(position == \"LHP\" | position == \"RHP\")\n\nTo illustrate what can happen with a sample, we now draw a sample. Let’s first set our computer’s random number seed so we get the same sample each time.\n\nset.seed(1599)\n\nThen draw a sample of 40 pitchers\n\npitchers_sample &lt;- pitchers |&gt;\n  sample_n(size = 40)\n\nand examine the mean difference in salary.\n\npitchers_sample |&gt;\n  group_by(position) |&gt;\n  summarize(salary_mean = mean(salary))\n\n# A tibble: 2 × 2\n  position salary_mean\n  &lt;chr&gt;          &lt;dbl&gt;\n1 LHP         9677309.\n2 RHP         3182428.\n\n\nThe left-handed pitchers make millions of dollars more per year! You can probably tell many stories why this might be the case. Maybe left-handed pitchers are needed by all teams, and there just aren’t many available because so few people are left-handed!\nWhat happens if we repeat this process many times? The figure below shows many repeated samples of size 40 from the population of pitchers.\n\n\n\n\n\n\n\n\n\nOur original result was really random noise: we happened by chance to draw a sample with some highly-paid left-handed pitchers!\nThis exercise illustrates what is known as the replication crisis: findings that are surprising in one sample may not hold in other repeated samples from the same population, or in the population as a whole. The replication crisis has many sources. One principal source is the one we illustrated above: sample-based estimates involve some randomness, and well-meaning researchers are (unfortunately) very good at telling interesting stories.\nOne solution to the replication crisis is to pay close attention to the statistical uncertainty in our estimates, such as that from random sampling. Another solution is to re-evaluate findings that are of interest on new samples. In any case, both the roots of the problem and the solutions are closely tied to sources of randomness in estimates, such as those generated using samples from a population.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#the-future-of-sample-surveys",
    "href": "topics/sampling.html#the-future-of-sample-surveys",
    "title": "Population Sampling",
    "section": "The future of sample surveys",
    "text": "The future of sample surveys\nSample surveys served as a cornerstone of social science research from the 1950s to the present. But there are concerns about their future:\n\nsome sampling frames, such as landline telephones, have become obsolete\nresponse rates have been falling for decades\nsample surveys are slower and more expensive than digital data\n\nWhat is the future for sample surveys? How can they be combined with other data?\nWe will close with a discussion of these questions, which you can also engage with in the Groves 2011 reading that follows this module.",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/sampling.html#footnotes",
    "href": "topics/sampling.html#footnotes",
    "title": "Population Sampling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, a simple random sample draws units independently with equal probabilities, and with replacement. Our sample is actually drawn without replacement. In an infinite population, the two are equivalent.↩︎",
    "crumbs": [
      " ",
      "Topics",
      "Population Sampling"
    ]
  },
  {
    "objectID": "topics/r_basics.html",
    "href": "topics/r_basics.html",
    "title": "R and RStudio",
    "section": "",
    "text": "This course uses R, RStudio, and Quarto\n\nR is statistical software\nRStudio is a user interface\nQuarto is an authoring framework for reproducible documents\n\nAll are free and open source. To set the software up on your computer, follow the steps in the Prerequisites section of R4DS. To learn about RStudio, visit the User Guide.\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "R and RStudio"
    ]
  },
  {
    "objectID": "topics/gender.html",
    "href": "topics/gender.html",
    "title": "Gender",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing Inequality",
      "Gender"
    ]
  },
  {
    "objectID": "topics/class.html",
    "href": "topics/class.html",
    "title": "Class",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing Inequality",
      "Class"
    ]
  },
  {
    "objectID": "topics/about.html",
    "href": "topics/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "Check the post pinned on Ed Discussion to see any week-specific changes to the schedule.\n\n\n\nPerson\nTime\nLocation\n\n\n\n\nStacy\nM 12–1pm\nRhodes 402\n\n\nJoanne\nM 1–2pm\nRhodes 408\n\n\nChang\nM 3–4pm\nRhodes 408\n\n\nIan\nT 10–11am\nGates 223\n\n\nLiz\nW 12:30–1:30pm\nRhodes 402\n\n\nDaniel\nW 1–2pm\nRhodes 406\n\n\nFede\nW 2–3pm\nRhodes 406\n\n\nOlaf\nW 3–4pm\nRhodes 406\n\n\nJonathan\nTh 11am–12pm\nRhodes 412\n\n\nAkira\nTh 3:30–4:30pm\nZoom\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "forms.html",
    "href": "forms.html",
    "title": "Forms",
    "section": "",
    "text": "If you are missing class, here is a form where you can tell us!\n\nAssignment extension form to request an assignment extension that does not count against flex days under exceptional circumstances\nExcused absence form. To request an excused absence from lecture or discussion\nLecture make-up form. For lectures that are missed which we haven’t excused\nFor non-excused absences from discussion, you should do the discussion activity on your own and submit in the Canvas assignment associated with that discussion\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/pset1.html",
    "href": "assignments/pset1.html",
    "title": "Problem Set 1: Visualization",
    "section": "",
    "text": "Due: 5pm on Wednesday, January 31.\n\n\n\n\n\n\nBefore you start\n\n\n\nCreate an anonymous identifier for yourself here! Want to see how you’ll be evaluated? Check out the rubric\n\n\nStudent identifer: [type your anonymous identifier here]\n\nUse this .qmd template to complete the problem set\nIn Canvas, you will upload the PDF produced by your .qmd file\nPut your identifier above, not your name! We want anonymous grading to be possible\n\nThis problem set involves both data analysis and reading.\n\nData analysis\nThis problem set uses the data lifeCourse.csv.\n\nlibrary(tidyverse)\nlibrary(scales)\nlifeCourse &lt;- read_csv(\"https://info3370.github.io/data/lifeCourse.csv\")\n\nThe data contain life course earnings profiles for four cohorts of American workers: those born in 1940, 1950, 1960, and 1970. Each row contains a summary of the annual earnings distribution for a particular birth cohort at a particular age, among the subgroup with a particular level of education. To prepare these data, we aggregated microdata from the Current Population Survey, provided through the Integrated Public Use Microdata Series.\nThe data contain five variables.\n\nquantity is the metric by which the earnings distribution is summarized: 10th, 50th, or 90th percentile\neducation is the educational subgroup being summarized: College Degree, Less than College\ncohort is the cohort (people with a given birth year) to which these data apply: 1940, 1950, 1960, 1970\nage is the age at which earnings were measured: 30–45\nincome is the value for the given earnings percentile in the given subgroup. Income values are provided in 2022 dollars\n\n\n\n1. Visualize (25 points)\nUse ggplot to visualize these data. To denote the different trajectories,\n\nmake your plot using geom_point() or geom_line()\nuse the x-axis for age\nuse the y-axis for income\nuse color for quantity\nuse facet_grid to make a panel of facets where each row is an education value and each column is a cohort value\n\nYou should prepare the graph as though you were going to publish it. Modify the axis titles so that a reader would know what is on the axis. Use appropriate capitalization in all labels. Try using the label_dollar() function from the scales package so that the y-axis uses dollar values.\nYour code should be well-formatted as defined by R4DS. In your produced PDF, no lines of code should run off the page.\nMany different graphs can be equally correct. You will be evaluated by\n\nhaving publication-ready graph aesthetics\ncode that follows style conventions\n\n\n# your code goes here\n\n\n\n2. Interpret (10 points)\nWrite 2-3 sentences summarizing the trends that you see in the data.\n2.1 (3 points). Focus on those born in 1970. For those with a college degree, how do the top and bottom of the income distribution change over the life course?\n\nType your answer here.\n\n2.2 (3 points). Focus on those born in 1970. How does the pattern differ for those without college degres differ from your answer in 2.1?\n\nType your answer here.\n\n2.3 (4 points). How do the patterns you identified in 2.1 and 2.2 change from the 1940 to the 1970 cohort?\n\nType your answer here.\n\n\n\n3. Connect to reading (15 points)\nRead p. 1–7 of following paper. Stop before the section “Analytic Framework for Decomposing Inequality.”\n\nCheng, Siwei. 2021. The shifting life course patterns of wage inequality.. Social Forces 100(1):1–28.\n\nOur data are not the same as Cheng’s. But our analysis is able to reproduce many of her findings. Answer each question in two sentences or less.\nCheng discusses period trends, cohort trends, and age trends.\n3.1 (3 points) Which dimension of your graph shows a cohort trend?\n\nType your answer here.\n\n3.2 (3 points) Which dimension of your graph shows an age trend?\n\nType your answer here.\n\n3.3 (3 points) Cheng discusses education-based cumulative advantage. Describe how you see this in your graph.\n\nType your answer here.\n\n3.4 (3 points) Cheng discusses within-education trajectory heterogeneity. Describe how your graph shows heterogeneity of outcomes within educational categories.\n\nType your answer here.\n\n3.5 (3 points) Cheng discusses wage volatility: how wages rise and fall over time for a given person. Why is our data (the Current Population Survey) the wrong dataset to study wage volatility?\n\nType your answer here.\n\n\n\nGrad question: Model-based estimates\nThis question assumes familiarity with Ordinary Least Squares.\n\nFor graduate students, this question is worth 20 points.\nFor undergraduate students, this question is optional and worth 0 points.\n\nThe data contain nonparametric estimates that contain some noise: the data points provided partly reflect random variation because they are estimatd in a sample.\nModel-based estimates reduce noise by pooling information across observations, at the cost of introducing assumptions. Fit an OLS model to the data using age as a numeric variable and education, cohort, and quantity as factor variables. Interact all of these predictors with each other.\n\nfit &lt;- lm(\n  income ~ age * factor(cohort) * education * quantity,\n  data = lifeCourse\n)\n\nEffectively, this estimates a best-fit line through each set of points depicted in your original figure. For each observation, store a prediction from this model (see predict()).\nRe-create your plot from (1) using\n\ngeom_point() for the nonparametric estimates (as above)\ngeom_line() for your model-based predicted values\n\n\n\nComputing environment\nLeave this at the bottom of your file, and it will record information such as your operating system, R version, and package versions. This is helpful for resolving any differences in results across people.\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.2.1    lubridate_1.9.3 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.3     purrr_1.0.2     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.4   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] bit_4.0.5         gtable_0.3.4      jsonlite_1.8.7    crayon_1.5.2     \n [5] compiler_4.3.2    tidyselect_1.2.0  parallel_4.3.2    yaml_2.3.7       \n [9] fastmap_1.1.1     R6_2.5.1          generics_0.1.3    knitr_1.44       \n[13] htmlwidgets_1.6.2 munsell_0.5.0     pillar_1.9.0      tzdb_0.4.0       \n[17] rlang_1.1.1       utf8_1.2.3        stringi_1.7.12    xfun_0.40        \n[21] bit64_4.0.5       timechange_0.2.0  cli_3.6.1         withr_2.5.1      \n[25] magrittr_2.0.3    digest_0.6.33     grid_4.3.2        vroom_1.6.4      \n[29] rstudioapi_0.15.0 hms_1.1.3         lifecycle_1.0.3   vctrs_0.6.4      \n[33] evaluate_0.22     glue_1.6.2        fansi_1.0.5       colorspace_2.1-0 \n[37] rmarkdown_2.25    tools_4.3.2       pkgconfig_2.0.3   htmltools_0.5.6.1\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The culmination of the course is a group research project. You will\n\ncreate your own research question\ndownload your own data\nvisualize the data\ninterpret your findings\n\nMore details on the project will come later in the semester.\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/pset0.html",
    "href": "assignments/pset0.html",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "",
    "text": "Due: 5pm on Wednesday, January 24.\nIn this exercise, you will install R, RStudio and Quarto. The exercise will ensure that they are set up correctly so that everybody is ready to dive into coursework.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#install-software-requirements",
    "href": "assignments/pset0.html#install-software-requirements",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Install Software Requirements",
    "text": "Install Software Requirements\nYou should follow the instructions in R4DS to:\n\nInstall R\nInstall RStudio\n\nOnce R and RStudio are successfully installed, this also means that Quarto is successfully installed because it comes bundled with RStudio.\nFinally, you will need to open RStudio, select the Terminal tab, and execute the following command: quarto install tinytex as demonstrated in the image below.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#download-problem-set",
    "href": "assignments/pset0.html#download-problem-set",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Download Problem Set",
    "text": "Download Problem Set\nNext, download this problem set which is a Quarto Markdown Document.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#render-the-problem-set",
    "href": "assignments/pset0.html#render-the-problem-set",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Render the Problem Set",
    "text": "Render the Problem Set\nFinally, open the downloaded problem set in RStudio. You should then click the “Render” button as shown in the image below.\n This will run the R code and combine the output with the text in the document into a PDF. By default, this PDF will be output in the same directory that you saved pset0.qmd in. You will then submit the PDF on Canvas.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#issues",
    "href": "assignments/pset0.html#issues",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Issues",
    "text": "Issues\nIf you run into issues while attempting to render the Problem Set, be sure to open a question on the Ed Discussion!",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset2.html",
    "href": "assignments/pset2.html",
    "title": "Problem Set 2: Data Transformation",
    "section": "",
    "text": "Due: 5pm on Wednesday, February 14.\nStudent identifer: [type your anonymous identifier here]\nThis problem set draws on the following paper.\nA note about sex and gender. As we have discussed in class, sex typically refers to categories assigned at birth (e.g., female, male). Gender is a performed construct with many possible values: man, woman, nonbinary, etc. The measure in the CPS-ASEC is “sex,” coded male or female. We will use these data to study sex disparities between those identifying as male and female. The paper at times uses “gender” to refer to this construct."
  },
  {
    "objectID": "assignments/pset2.html#data-analysis-existing-question",
    "href": "assignments/pset2.html#data-analysis-existing-question",
    "title": "Problem Set 2: Data Transformation",
    "section": "1. Data analysis: Existing question",
    "text": "1. Data analysis: Existing question\n20 points. Reproduce Figure 1 from the paper.\nVisit cps.ipums.org to download data from the 1962–2023 March Annual Social and Economic Supplement. Include these variables in your cart: sex, age, asecwt, empstat.\nTo reduce extract size, select cases to those ages 25–54. Before submitting your extract, we recommend changing the data format to “Stata (.dta)” so that you get value labels.\n\n\n\n\n\n\nTip\n\n\n\nLook ahead: you will later study a new outcome of your own choosing. You could add it to your cart now if you want.\n\n\nOn your computer, analyze these data.\n\nfilter to asecwt &gt; 0 (see paper footnote on p. 6995 about negative weights)\nmutate to create an employed variable indicating that empstat == 10 | empstat == 12\nmutate to convert sex to a factor variable using as_factor\ngroup by sex and year\nsummarize the proportion employed: use weighted.mean to take the mean of employed using the weight asecwt\n\nYour figure will be close but not identical to the original. Yours will include some years that the original did not. Feel free to change aesthetics of the plot, such as the words used in labels. For example, it would be more accurate to the data to label the legend “Sex” with values “Male” and “Female.”\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(haven)"
  },
  {
    "objectID": "assignments/pset2.html#reading-questions",
    "href": "assignments/pset2.html#reading-questions",
    "title": "Problem Set 2: Data Transformation",
    "section": "2. Reading questions",
    "text": "2. Reading questions\n2.1 (3 points) The authors write that “change in the gender system has been deeply asymmetric.” Explain this in a sentence or two to someone who hasn’t read the article.\n2.2 (3 points) The authors discuss cultural changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about cultural changes.\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure how to word a survey question, here are some examples from the American Time Use Survey, Current Population Survey, and General Social Survey.\n\n\n2.3 (3 points) The authors discuss institutional changes that could lead to greater equality. Propose a question that could (hypothetically) be included in the CPS-ASEC questionnaire to help answer questions about institutional changes.\n2.4 (1 point) What was one fact presented in this paper that most surprised you?"
  },
  {
    "objectID": "assignments/pset2.html#a-new-outcome",
    "href": "assignments/pset2.html#a-new-outcome",
    "title": "Problem Set 2: Data Transformation",
    "section": "3. A new outcome",
    "text": "3. A new outcome\n20 points. The CPS-ASEC has numerous variables. Pick another variable of your choosing. Add it to your cart in IPUMS, and visualize how that variable has changed over time for those identifying as male and female.\nAs in the previous plot, year should be on the x-axis and color should represent sex. The y-axis is up to you. You can examine something like median income, proportion holding college degrees, or the 90th percentile of usual weekly work hours. You can restrict to some subset if you want, such as those who are employed.\nYour answer should include\n\na written statement of what you estimated: the variable you chose, any sample restrictions you made, and how you summarized that variable\na written interpretation of what you found\ncode following style conventions\nyour publication-quality visualization"
  },
  {
    "objectID": "assignments/pset2.html#grad-question-ratio-and-difference",
    "href": "assignments/pset2.html#grad-question-ratio-and-difference",
    "title": "Problem Set 2: Data Transformation",
    "section": "Grad question: Ratio and difference",
    "text": "Grad question: Ratio and difference\n\nThis question is required for grad students. It is optional for undergrads, and worth no extra credit.\n\n20 points. The figures above visualize a summary statistic for each subgroup: male and female. Another way to visualize this is with the ratio (female statistic / male statistic) or difference of the two (female statistic - male statistic).\nFor your own question, produce two new visualizations:\n\none showing the female / male ratio over time\none showing the female - male difference over time\n\nWhich do you find easier to interpret, and why?\n\n\n\n\n\n\nTip\n\n\n\nIt is likely that your code for the previous parts produced one column with estimates and another column sex containing the values male and female. One way to calculate a ratio and difference is to reshape the data so that there is one row for each year, containing both the male and female estimates. You can do this with pivot_wider where the names come from sex and the values come from the column containing your estimates. Then you can mutate() to create the ratio and difference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Studying Social Inequality with Data Science",
    "section": "",
    "text": "Together, we will generate new knowledge about social inequality using the tools of data science",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Studying Social Inequality with Data Science",
    "section": "Learning goals",
    "text": "Learning goals\nAs a result of participating in this course, students will be able to\n\nvisualize economic inequality with graphs that summarize survey data\nconnect theories about inequality to quantitative empirical evidence\nevaluate the effects of hypothetical interventions to reduce inequality\nconduct data analysis using the R programming language",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Some assignments have associated readings, which are listed in the reading column. Due dates are tentative for assignments that are not yet released.\n\n\n\nDue date\nAssignment\nReading\n\n\n\n\nWed Jan 24 at 5pm\nProblem Set 0\n\n\n\nWed Jan 31 at 5pm\nProblem Set 1\nCheng 2021\n\n\nWed Feb 7 at 5pm\nPeer Review 1\n\n\n\nWed Feb 14 at 5pm\nProblem Set 2\nEngland et al. 2020\n\n\nWed Feb 21 at 5pm\nPeer Review 2\n\n\n\nFebruary break\n\n\n\n\nWed Mar 6 at 5pm\nProblem Set 3\n\n\n\nWed Mar 13 at 5pm\nPeer Review 3\n\n\n\nWed Mar 20 at 5pm\nProblem Set 4\n\n\n\nWed Mar 27 at 5pm\nPeer Review 4\n\n\n\nSpring break\n\n\n\n\nWed Apr 17 at 5pm\nProblem Set 5\n\n\n\nWed Apr 24 at 5pm\nProject writeup + presentation slides due\n\n\n\nTh Apr 25 8:40–9:55am\nProject presentations in discussion\n\n\n\nWed May 1 at 5pm\nPeer Review 5\n\n\n\nTh May 2 8:40–9:55am\nProject presentations in discussion"
  },
  {
    "objectID": "schedule.html#assignments",
    "href": "schedule.html#assignments",
    "title": "Schedule",
    "section": "",
    "text": "Some assignments have associated readings, which are listed in the reading column. Due dates are tentative for assignments that are not yet released.\n\n\n\nDue date\nAssignment\nReading\n\n\n\n\nWed Jan 24 at 5pm\nProblem Set 0\n\n\n\nWed Jan 31 at 5pm\nProblem Set 1\nCheng 2021\n\n\nWed Feb 7 at 5pm\nPeer Review 1\n\n\n\nWed Feb 14 at 5pm\nProblem Set 2\nEngland et al. 2020\n\n\nWed Feb 21 at 5pm\nPeer Review 2\n\n\n\nFebruary break\n\n\n\n\nWed Mar 6 at 5pm\nProblem Set 3\n\n\n\nWed Mar 13 at 5pm\nPeer Review 3\n\n\n\nWed Mar 20 at 5pm\nProblem Set 4\n\n\n\nWed Mar 27 at 5pm\nPeer Review 4\n\n\n\nSpring break\n\n\n\n\nWed Apr 17 at 5pm\nProblem Set 5\n\n\n\nWed Apr 24 at 5pm\nProject writeup + presentation slides due\n\n\n\nTh Apr 25 8:40–9:55am\nProject presentations in discussion\n\n\n\nWed May 1 at 5pm\nPeer Review 5\n\n\n\nTh May 2 8:40–9:55am\nProject presentations in discussion"
  },
  {
    "objectID": "schedule.html#meetings",
    "href": "schedule.html#meetings",
    "title": "Schedule",
    "section": "Meetings",
    "text": "Meetings\nReadings are to be read after class, unless otherwise specified. Readings from R for Data Science (R4DS, Wickham et al. 2023) are intended as references that do not need to be read line-by-line but are a useful reference to support ideas.\n\n\n\nDate\nType\nTopic\nReading\n\n\n\n\nJan 23\nLecture\nWelcome\nJencks 2002 p. 49–53\n\n\nJan 25\nDiscussion\nVisualization\nR4DS Ch 1\n\n\nJan 30\nLecture\nSampling: Simple random and unequal probability\nProbability sampling\n\n\nFeb 1\nDiscussion\nData transformation (day 1 / 2)\nR4DS Ch 3\n\n\nFeb 6\nLecture\nSampling: Stratified, clustered, and the future\nGroves 2011\n\n\nFeb 8\nDiscussion\nData transformation (day 2 / 2)\nR4DS Ch 3\n\n\nFeb 13\nLecture\nStatistical learning\nBerk 2020 Ch 1 p. 1–5, stopping at paragraph ending “…is nonlinear.” Then p. 14–17 “Model misspecification…” through “…will always be in play.”\n\n\nFeb 15\nDiscussion\nStatistical learning activity\n\n\n\nFeb 20\nLecture\nPredicting life outcomes\nOptional: Salganik et al. 2020\n\n\nFeb 22\nDiscussion\nPSID Income Prediction Challenge (day 1 / 2)\n\n\n\nFebruary break\n\n\n\n\n\nFeb 29\nDiscussion\nPSID Income Prediction Challenge (day 2 / 2)\n\n\n\nMar 5\nLecture\nPSID Income Prediction: Awards and takeaways\nOptional: Lundberg et al. 2024\n\n\nMar 7\nDiscussion\nRacial wealth gap\n\n\n\nMar 12\nLecture\nPolitical origins of racial inequality\n\n\n\nMar 14\nDiscussion\nClass inequality exercise\n\n\n\nMar 19\nLecture\nClass + gender inequality\n\n\n\nMar 21\nDiscussion\nProject work\n\n\n\nMar 26\nLecture\nAsking research questions\n\n\n\nMar 28\nDiscussion\nProject work\n\n\n\nSpring break\n\n\n\n\n\nApr 9\nLecture\nMoral arguments\nBefore class, watch intro to Rawls and 3:49–5:05 on Nozick\n\n\nApr 11\nDiscussion\nProject work\n\n\n\nApr 16\nLecture\nReducing inequality: Educational expansion\nBrand 2023 Ch 1\n\n\nApr 18\nDiscussion\nProject work\n\n\n\nApr 23\nLecture\nReducing inequality: Reparations\nBefore class, watch Ta-Nehisi Coates\n\n\nApr 25\nDiscussion\nProject presentations\n\n\n\nApr 30\nLecture\nReducing inequality: Sentencing reform\nBefore class, explore A Matter of Time\n\n\nMay 2\nDiscussion\nProject presentations\n\n\n\nMay 7\nCourse summary"
  },
  {
    "objectID": "topics/asking_questions.html",
    "href": "topics/asking_questions.html",
    "title": "Asking Questions",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Asking Questions"
    ]
  },
  {
    "objectID": "topics/data_transformation.html",
    "href": "topics/data_transformation.html",
    "title": "Data transformation",
    "section": "",
    "text": "This exercise examines how income inequality has changed over time in the U.S. We will measure inequality by the 10th, 50th, and 90th percentiles of wage and salary income from 1962 to 2022. We expect that this exercise may take more time than one discussion session.1 You will begin by downloading data and end by making this graph.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#data-access",
    "href": "topics/data_transformation.html#data-access",
    "title": "Data transformation",
    "section": "Data access",
    "text": "Data access\nThis exercise uses data from the Current Population Survey.\n\nRegister for an account at cps.ipums.org\nLog in\nClick “Get Data”\nAdd the following variables to your cart: incwage, educ, wkswork2, age, asecwt\nAdd the 1962–2023 ASEC samples to your cart. Exclude the basic monthly samples\nCreate a data extract\n\nSelect cases to only download people ages 30–45\nChoose to download in Stata (.dta) format\n\nSubmit your extract and download the data!\n\nStore your data in a working directory: a folder on your computer that will hold the data for this exercise.\n\n\n\n\n\n\nTip\n\n\n\nKeep a browser tab open with the IPUMS webpage to easily access full documentation",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#prepare-r-environment",
    "href": "topics/data_transformation.html#prepare-r-environment",
    "title": "Data transformation",
    "section": "Prepare R environment",
    "text": "Prepare R environment\nIn RStudio, create a Quarto document. Save it in your working directory.\nUse the code below to load packages:\n\nlibrary(tidyverse)\nlibrary(haven)\n\nThe haven package allows us to load data in the .dta format designed for Stata. Use read_dta()) and store the data in an object called micro. By default, these data are stored in a tibble.\n\nmicro &lt;- read_dta(\"cps_00077.dta\")\n\n\n\n\n\n\n\nTip\n\n\n\n\nChange the file name to the name of the file you downloaded\nIf R says the file does not exist in your current working directory, you may need to set your working directory by clicking Session -&gt; Set Working Directory -&gt; To Source File Location on a Mac or Tools -&gt; Change Working Directory on Windows.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#get-familiar-with-our-dataset",
    "href": "topics/data_transformation.html#get-familiar-with-our-dataset",
    "title": "Data transformation",
    "section": "Get familiar with our dataset",
    "text": "Get familiar with our dataset\nType micro in the console. What do you see?\n\nHow many rows are there?\nHow many columns?\n\nSome columns such as educ have a numeric code and a label. The code is how IPUMS has stored the data. The label is what the code means. Getting these labels is a benefit of downloading the file in .dta format.\nType View(micro) in the console. This will pop up another tab in RStudio which allows you to scroll through the dataset. You can see that each column name has a description. For instance asecwt is “annual social and economic supplement weight.”",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#filter-to-cases-of-interest",
    "href": "topics/data_transformation.html#filter-to-cases-of-interest",
    "title": "Data transformation",
    "section": "filter() to cases of interest",
    "text": "filter() to cases of interest\n\nIn this step, you will use filter() to convert your micro object to a new object called filtered.\n\nThe filter() function keeps only rows in our dataset that correspond to those we want to study. The examples on the documentation page are especially helpful. The R4DS section is also helpful.\nHere are two ways to use filter() to restrict to people working 50+ weeks per year. One way is to call the filter() function and hand it two arguments\n\n.data = micro is the dataset\nyear == 1962 is a logical condition coded TRUE for observations in 1962\n\n\nfilter(.data = micro, year == 1962)\n\nThe result of this call is a tibble with only the observations from 1962. Another way to do the same operation is with the pipe operator |&gt;\n\nmicro |&gt;\n  filter(year == 1962)\n\nThis approach begins with the data set micro. The pipe operator |&gt; hands this data set on as the first argument to the filter() function in the next line. As before, the second argument is the logical condition year == 1962.\nThe piping approach is often preferable because it reads like a sentence: begin with data, then filter to cases with a given condition. The pipe is also useful\nThe pipe operator |&gt; takes what is on the first line and hands it on as the first argument to the function in the next line. This reads in a sentence: begin with the micro tibble and then filter() to cases with year == 1962. The pipe can also string together many operations, with comments allowed between them:\n\nmicro |&gt;\n  # Restrict to 1962\n  filter(year == 1962) |&gt;\n  # Restrict to ages 40-44\n  filter(age &gt;= 40 & age &lt;= 44)\n\nYour turn. Begin with the micro dataset. Filter to\n\npeople working 50+ weeks per year (check documentation for wkswork2)\nvalid report of incwage greater than 0 and less than 99999998\n\nIf you get stuck, see how we did it at the end of this page. filtered should have 1,350,542 rows and 15 columns.\n\n\n\n\n\n\nNote\n\n\n\nFiltering can be a dangerous business! For example, above we dropped people with missing values of income. But what if the lowest-income people refuse to answer the income question? We often have no choice but to filter to those with valid responses, but you should always read the documentation to be sure you understand who you are dropping and why.\n\n\n\n\n\nfilter() without the pipe\n\n\n\n\nfilter() with the pipe",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "href": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "title": "Data transformation",
    "section": "group_by() and summarize() for subpopulation summaries",
    "text": "group_by() and summarize() for subpopulation summaries\n\nIn this step, you will use group_by() and summarize() to convert your mutated object to a new object called summarized.\n\nEach row in our dataset is a person. We want a dataset where each row is a year.\n\nUse group_by() function to group by year. The next operations will automatically be carried out within groups\nUse summarize() to aggregate to the 10th, 50th, and 90th percentiles within each year\n\np10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1)\np50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5)\np90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9)\n\n\nIf you get stuck, see how we did it at the end of this page.\n\n\n\n\n\n\nTip\n\n\n\nThat was a new way of calling a package! The Hmisc package has a bunch of miscellaneous functions. To install the package, type install.packages(\"Hmisc\"). One of the functions is wtd.quantile(), which summarizes data with weighted quantiles (e.g., the 10th percentile estimated in a survey with sampling weights). So why call it with Hmisc::wtd.quantile() instead of using library(Hmisc)? When you use library(), you load all the functions in a package. Hmisc is a big package, and some of the functions have the same names as other functions we use in tidyverse. Loading the whole package can create conflicts where two function shave the same name! Instead, the way we’ve written it above tells R to just look in the package for the particular function we’re using at that moment.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe summarized data frame should have 62 rows and 4 columns (year, education categories, p10, p50, and p90).\n\n\n\n\n\nSample summaries\n\n\n\n\nWeighted sample summaries",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "href": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "title": "Data transformation",
    "section": "pivot_longer() to reshape data",
    "text": "pivot_longer() to reshape data\n\nIn this step, you will use pivot_longer() to convert your summarized object to a new object called pivoted. We first explain why, then explain the task.\n\nWe ultimately want to make a ggplot() where income values are placed on the y-axis. We want to plot the 10th, 50th, and 90th percentiles along this axis, distinguished by color. We need them all in one colun! But currently, they are in three columns.\nHere is the task. How our data look:\n\n\n# A tibble: 62 × 4\n   year   p10   p50   p90\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1962  2000  5100  9000\n2  1963  2000  5200  9200\n# ℹ 60 more rows\n\n\nHere we want our data to look:\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10        2000\n2  1962 p50        5100\n3  1962 p90        9000\n4  1963 p10        2000\n5  1963 p50        5200\n6  1963 p90        9200\n# ℹ 180 more rows\n\n\nThis way, we can use year for the x-axis, quantity for color, and value for the y-axis.\nUse pivot_longer() to change the first data frame to the second.\n\nUse the cols argument to tell it which columns will disappear\nUse the names_to argument to tell R that the names of those variables will be moved to a column called quantity\nUse the values_to argument to tell R that the values of those variables will be moved to a column called income\n\nIf you get stuck, see how we did it at the end of this page.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "href": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "title": "Data transformation",
    "section": "left_join() an inflation adjustment",
    "text": "left_join() an inflation adjustment\n\nIn this step, you will use left_join() to merge in an inflation adjustment\n\nA dollar in 1962 bought a lot more than a dollar in 2022. We will adjust for inflation using the Consumer Price Index, which tracks the cost of a standard basket of market goods. We already took this index to create a file inflation.csv,\n\ninflation &lt;- read_csv(\"https://info3370.github.io/data/inflation.csv\")\n\n\n\n# A tibble: 62 × 2\n   year inflation_factor\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1962            10.1 \n2  1963             9.95\n3  1964             9.82\n# ℹ 59 more rows\n\n\nThe inflation_factor tells us that $1 in 1962 could buy about as much as $10.10 in 2023. To take a 1962 income and report it in 2023 dollars, we should multiple it by 10.1. We need to join our data\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10        2000\n2  1962 p50        5100\n3  1962 p90        9000\n# ℹ 183 more rows\n\n\ntogether with inflation.csv by the linking variable year. Use left_join() to merge inflation_factor onto the dataset pivoted. Below is a hypothetical example for the structure.\n\n# Hypothetical example\njoined &lt;- data_A |&gt;\n  left_join(\n    data_B,\n    by = join_by(key_variable_in_A_and_B)\n  )\n\nIf you get stuck, see how we did it at the end of this page.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "href": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "title": "Data transformation",
    "section": "mutate() to adjust for inflation",
    "text": "mutate() to adjust for inflation\n\nIn this step, you will use mutate() to multiple income by the inflation_factor\n\nThe mutate() function modifies columns. It can overwrite existing columns or create new columns at the right of the data set. The new variable is some transformation of the old variables.\n\n# Hypothetical example\nold_data |&gt;\n  mutate(new_variable = old_variable_1 + old_variable_2)\n\nUse mutate() to modify income so that it takes the values income * inflation_factor. If you get stuck, see how we did it at the end of this page.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#ggplot-to-visualize",
    "href": "topics/data_transformation.html#ggplot-to-visualize",
    "title": "Data transformation",
    "section": "ggplot() to visualize",
    "text": "ggplot() to visualize\nNow make a ggplot() where\n\nyear is on the x-axis\nincome is on the y-axis\nquantity is denoted by color\n\nDiscuss. What do you see in this plot?",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#all-together",
    "href": "topics/data_transformation.html#all-together",
    "title": "Data transformation",
    "section": "All together",
    "text": "All together\nPutting it all together, we have a pipeline that goes from data to the plot.\n\nread_dta(\"cps_00077.dta\") |&gt;\n  # Subset to cases working full year\n  filter(wkswork2 == 6) |&gt;\n  # Subset to cases with valid income\n  filter(incwage &gt; 0 & incwage &lt; 99999998) |&gt;\n  # Produce summaries\n  group_by(year) |&gt;\n  summarize(\n    p10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1),\n    p50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5),\n    p90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9\n    ),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = c(\"p10\",\"p50\",\"p90\"),\n    names_to = \"quantity\",\n    values_to = \"income\"\n  ) |&gt;\n  # Join data for inflation adjustment\n  left_join(\n    read_csv(\"inflation.csv\"),\n    by = join_by(year)\n  ) |&gt;\n  # Apply the inflation adjustment\n  mutate(income = income * inflation_factor) |&gt;\n  # Produce a ggplot\n  ggplot(aes(x = year, y = income, color = quantity)) +\n  geom_line() +\n  xlab(\"Year\") +\n  scale_y_continuous(name = \"Annual Wage and Salary Income\\n(2023 dollars)\",\n                     labels = scales::label_dollar()) +\n  scale_color_discrete(name = \"Percentile of\\nDistribution\",\n                       labels = function(x) paste0(gsub(\"p\",\"\",x),\"th\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#finished-early",
    "href": "topics/data_transformation.html#finished-early",
    "title": "Data transformation",
    "section": "Finished early?",
    "text": "Finished early?\nIf you are finished early, you could\n\nincorporate the educ variable in your plot. You might want to group by those who do and do not hold college degrees, perhaps using facet_grid()\ntry geom_histogram() for people’s incomes in a specific year\nexplore IPUMS-CPS for other variables and begin your own visualization",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#footnotes",
    "href": "topics/data_transformation.html#footnotes",
    "title": "Data transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to past TA Abby Sachar for designing the base of this exercise.↩︎",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/prediction.html",
    "href": "topics/prediction.html",
    "title": "Predicting life outcomes",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing Inequality",
      "Predicting life outcomes"
    ]
  },
  {
    "objectID": "topics/race.html",
    "href": "topics/race.html",
    "title": "Race",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing Inequality",
      "Race"
    ]
  },
  {
    "objectID": "topics/visualization.html",
    "href": "topics/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Prerequisites. You should first install R and RStudio as described in the R4DS Prerequisites. If you are unfamiliar with the layout of RStudio, see the User Guide.\nIn this discussion section, we will",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-the-environment",
    "href": "topics/visualization.html#prepare-the-environment",
    "title": "Visualization",
    "section": "Prepare the environment",
    "text": "Prepare the environment\nOpen a new R Script by clicking the button at the top left of RStudio. Save your R Script in a folder you will use for this exercise.\nPaste the code below into your R Script. Place your cursor within the line and hit CMD + Enter or CTRL + Enter to run the code and load the tidyverse package.\n\nlibrary(tidyverse)\n\nYou will see action in the console. You have added some functionality to R for this session!\nThe data can be loaded from the course website with the line below.\n\ndata &lt;- read_csv(file = \"https://info3370.github.io/data/jencks_table1.csv\")\n\nWhen you run this code, the object data will appear in your environment pane.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#explore-the-data",
    "href": "topics/visualization.html#explore-the-data",
    "title": "Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nType data in your console. You can see the data!\n\ncountry country name\nratio ratio of 90th to 10th percentile of household income. You can think of this as how many dollars a high-income household receives for each dollar that a low-income household receives\ngdp Gross Domestic Product Per Capita, expressed as a proportion of U.S. GDP\nlife_expectancy life expectancy at birth\n\nFor details on the data, see Jencks (2002) Table 1.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#produce-a-graph",
    "href": "topics/visualization.html#produce-a-graph",
    "title": "Visualization",
    "section": "Produce a graph",
    "text": "Produce a graph\nWe are ready to produce a graph! The code below will produce a simple graph.\n\ndata |&gt;\n  ggplot(mapping = aes(x = ratio, y = gdp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nLet’s break this code down into pieces\n\ndata tells R to start with the object data\n|&gt; is called the pipe operator. It passes the data object down to the function in the next line\nggplot() is a function that creates a plot environment\nthe argument mapping = aes(x = ratio, y = gdp) tells ggplot() how variables in the data will correspond to elements of the plot. We will visualize ratio on the x-axis and gdp on the y-axis\n+ tells ggplot() we will add a new layer on the next line\ngeom_point() tells ggplot() to add a layer of points to the graph",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#customizing-your-graph",
    "href": "topics/visualization.html#customizing-your-graph",
    "title": "Visualization",
    "section": "Customizing your graph",
    "text": "Customizing your graph\nIn your group, create additional layers with additional lines connected by +. Be creative! Here are some ideas:\n\nlabel the axes with scale_x_continuous(name = \"your text here\") and scale_y_continuous(name = \"your text here\")\nlabel countries using geom_text or geom_text_repel, with the aesthetic label = country\n\nThere are many possible graphs to make. An example is below.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#interpret-your-graph",
    "href": "topics/visualization.html#interpret-your-graph",
    "title": "Visualization",
    "section": "Interpret your graph",
    "text": "Interpret your graph\nOnce you are happy with your graph,\n\nwrite a few sentences explaining your graph\ndiscuss what questions you would like to ask next",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-a-quarto-report",
    "href": "topics/visualization.html#prepare-a-quarto-report",
    "title": "Visualization",
    "section": "Prepare a Quarto report",
    "text": "Prepare a Quarto report\nCreate a new Quarto document. Put your R code and interpretation into that document. Upload to Canvas!",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/welcome.html",
    "href": "topics/welcome.html",
    "title": "Welcome!",
    "section": "",
    "text": "[Slides]\nAmerica is more unequal by many measures than other industrialized nations. Should we be concerned? In today’s reading, Jencks discusses how quantitative empirical evidence on inequality relates to arguments about the amount of inequality that is desirable.\nAfter our discussion in class, read p. 49–53 of this paper:\nThe paper’s engagement with both normative arguments and quantitative evidence is an example we will follow this semester.\nUltimately, this class is creative: you will produce new evidence about a question of your choosing. To get there, we first need to learn a bit about population data and how to work with those data.",
    "crumbs": [
      " ",
      "Topics",
      "Welcome!"
    ]
  },
  {
    "objectID": "topics/welcome.html#summary-video-what-we-covered-today",
    "href": "topics/welcome.html#summary-video-what-we-covered-today",
    "title": "Welcome!",
    "section": "Summary video: What we covered today",
    "text": "Summary video: What we covered today",
    "crumbs": [
      " ",
      "Topics",
      "Welcome!"
    ]
  }
]