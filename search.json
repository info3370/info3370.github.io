[
  {
    "objectID": "who_we_are.html",
    "href": "who_we_are.html",
    "title": "Who We Are",
    "section": "",
    "text": "Get to know a little bit about our teaching team! For office hours, see the Syllabus.\n\n\n\n\n\n\n\n\nIan Lundberg\nilundberg@cornell.edu\n(he / him)\nWorking with data to understand inequality brings me joy and meaning, as I first discovered as a college student years ago. I hope to share that joy with you! Other joys of mine include hiking, surfing, and oatmeal with blueberries.\n\n\n\n\n\n\n\n\nFederica Bologna\nfb265@cornell.edu\n(she / her)\nI am passionate about using data to support patient-centered care and study gender inequality in fiction readership. I love binge-watching shows, hiking, and making food with friends. Excited to meet you in class!\n\n\n\n\n\nDaniel Molitor\ndmolitor@infosci.cornell.edu\nMy research interests primarily focus on applying computational methods to important policy questions. Developing software and building tools is also something I love to do! In my free time I enjoy doing pretty much anything that’s active and outdoors and I particularly enjoy playing tennis, kayaking, and fishing.\n\n\n\n\n\n\n\n\nStacy Boey\n(she / her)\nHi everyone! I am an information science major concentrating in data science and UX. Some hobbies of mine include crocheting, thrifting, and cooking. I look forward to a great semester with you guys!\n\n\n\n\n\nChang Chen\nHi everyone! I study biometry and information science, concentrating on data science. I enjoy playing badminton and cooking. I look forward to working with all of you!\n\n\n\n\n\nOlaf de Rohan Willner\nHello everyone! I study Information Science and Government and am fascinated by understanding the world and how policies shape it through data. Outside of class I love watching 1950’s French movies, running, and cooking. Looking forward to meeting you all this semester!\n\n\n\n\n\nAkira Goh\nHi everyone, I am a Dyson and Infoscience double major. I love scubadiving, snowboarding, and working out. Excited to be working with all of you.\n\n\n\n\n\nJonathan Gotian\n(he / him)\nHi all, I am a Dyson major and pursuing an Information Science minor studying business analytics and data science. In my free time, I love playing squash and chess. Excited to work with you all this semester!\n\n\n\n\n\nJoanne/Xinyu Hu\n(she / they)\nHey all! I am an Information Science and Communication double major. Thinking about socioeconomic inequalities is always an important framework for me to see the world, and looking forward to working with you all in the class! Outside schoolwork, I enjoy biking, exploring storytelling, and visiting museums (LA The Broad is my favorite in the US so far but always open to recommendations)\n\n\n\n\n\nLiz Moon\nHi there! I study Information and Computer Science with a focus on Data Science. I am passionate about using data-driven solutions to solve social science challenges. My other hobbies include experimenting with fusion cooking and film analysis. Looking forward to another great semester with everyone!"
  },
  {
    "objectID": "topics/weights.html",
    "href": "topics/weights.html",
    "title": "Weights",
    "section": "",
    "text": "When studying population-level inequality, our goal is to draw inference about all units in the population. We want to know about the people in the U.S., not just the people who answer the Current Population Survey. Drawing inference from a sample to a population is most straightforward for a simple random sample: when people are chosen at random with equal probabilities. For simple random samples, the sample average of any variable is an unbiased and consistent estimator of the population average.\nBut the Current Population Survey is not a simple random sample. Neither are most labor force samples! These samples still begin with a sampling frame, but people are chosen with unequal probabilities. We need sample weights to address this fact.\nIn the CPS, a key goal is to estimate unemployment in each state. Every state needs to have enough sample size—even tiny states like Wyoming. In order to make those estimates, the CPS oversamples people who live in small states.\nTo draw good population inference, our analysis must incorporate what we know about how the data were collected. If we ignore the weights, our sample will have too many people from Wyoming and too few people from California. Weights correct for this.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#how-survey-designers-create-weights",
    "href": "topics/weights.html#how-survey-designers-create-weights",
    "title": "Weights",
    "section": "How survey designers create weights",
    "text": "How survey designers create weights\nTo calculate sampling weight on person \\(i\\), those who design survey samples take the ratio \\[\\text{weight on unit }i = \\frac{1}{\\text{probability of including person }i\\text{ in the sample}}\\] You can think of the sampling weight as the number of population members a given sample member represents. If there are 100 people with a 1% chance of inclusion, then on average 1 of them will be in the sample. That person represents \\(\\frac{1}{.01}=100\\) people.\n\n\n\n\n\n\nExample redux: California and Wyoming\n\n\n\nSuppose Californians are sampled with probability 0.0004. Then each Californian represents 1 / 0.0004 = 2,500 people. Each Californian should receive a weight of 2,500. Working out the same math for Wyoming, each Wyoming resident should receive a weight of 250. The total weight on these two samples will then be proportional to the sizes of these two populations.\n\n\nIn practice, weighting is more complicated: survey administrators adjust weights for differential nonresponse across population subgroups (a method called post-stratification). How to construct weights is beyond the scope of this course, and could be a whole course in itself!",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#point-estimates",
    "href": "topics/weights.html#point-estimates",
    "title": "Weights",
    "section": "Point estimates",
    "text": "Point estimates\nWhen we download data, we typically download a column of weights. For simplicity, suppose we are given a sample of four people. The weight column tells us how many people in the population each person represents. The employed column tells us whether each person employed.\n\n\n     name weight employed\n1    Luis      4        1\n2 William      1        0\n3   Susan      1        0\n4  Ayesha      4        1\n\n\nIf we take an unweighted mean, we would conclude that only 50% of the population is employed. But with a weighted mean, we would conclude that 80% of the population is employed! This might be the case if the sample was designed to oversample people at a high risk of unemployment.\n\n\n\n\n\n\n\n\n\nEstimator\nMath\nExample\nResult\n\n\n\n\nUnweighted mean\n\\(=\\frac{\\sum_{i=1}^n Y_i}{n}\\)\n\\(=\\frac{1 + 0 + 0 + 1}{4}\\)\n= 50% employed\n\n\nWeighted mean\n\\(=\\frac{\\sum_{i=1}^n w_iY_i}{\\sum_{i=1}^n w_i}\\)\n\\(=\\frac{4*1 + 1*0 + 1*0 + 4*1}{4 + 1 + 1 + 4}\\)\n= 80% employed\n\n\n\nIn R, the weighted.mean(x, w) function will calculate weighted means where x is an argument for the outcome variable and w is an argument for the weight variable.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#standard-errors",
    "href": "topics/weights.html#standard-errors",
    "title": "Weights",
    "section": "Standard errors",
    "text": "Standard errors\nAs you know from statistics, our sample mean is unlikely to equal the population mean. There is random variation in which people were chosen for inclusion in our sample, and this means that across hypothetical repeated samples we would get different sample means! You likely learned formulas to create a standard errors, which quantifies how much a sample estimator would move around across repeated samples.\nUnfortunately, the formula you learned doesn’t work for complex survey samples! Simple random samples (for which those formulas hold) are actually quite rare. When you face a complex survey sample, those who administer the survey might provide\n\na vector of \\(n\\) weights for making a point estimate\na matrix of \\(n\\times k\\) replicate weights for making standard errors\n\nBy providing \\(k\\) different ways to up- and down-weight various observations, the replicate weights enable you to generate \\(k\\) estimates that vary in a way that mimics how the estimator might vary if applied to different samples from the population. For instance, our employment sample might come with 3 replicate weights.\n\n\n     name weight employed repwt1 repwt2 repwt3\n1    Luis      4        1      3      5      3\n2 William      1        0      1      2      2\n3   Susan      1        0      3      1      1\n4  Ayesha      4        1      5      3      4\n\n\nThe procedure to use replicate weights depends on how they are constructed. Often, it is relatively straightforward:\n\nuse weight to create a point estimate \\(\\hat\\tau\\)\nuse repwt* to generate \\(k\\) replicate estimates \\(\\hat\\tau^*_1,\\dots,\\hat\\tau^*_k\\)\ncalculate the standard error of \\(\\hat\\tau\\) using the replicate estimates \\(\\hat\\tau^*\\). The formula will depend on how the replicate weights were constructed, but it will likely involve the standard deviation of the \\(\\hat\\tau^*\\) multiplied by some factor\nconstruct a confidence interval1 by a normal approximation \\[(\\text{point estimate}) \\pm 1.96 * (\\text{standard error estimate})\\]\n\nIn our concrete example, the point estimate is 80% employed. The replicate estimates are 0.67, 0.73, 0.70. Variation across the replicate estimates tells us something about how the estimate would vary across hypothetical repeated samples from the population.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#computational-strategy-for-replicate-weights",
    "href": "topics/weights.html#computational-strategy-for-replicate-weights",
    "title": "Weights",
    "section": "Computational strategy for replicate weights",
    "text": "Computational strategy for replicate weights\nUsing replicate weights can be computationally tricky! It becomes much easier if you write an estimator() function. Your function accepts two arguments\n\ndata is the tibble containing the data\nweight_name is the name of a column containing the weight to be used (e.g., “repwt1”)\n\nExample. If our estimator is the weighted mean of employment,\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    summarize(\n      estimate = weighted.mean(\n        x = employed,\n        # extract the weight column\n        w = sim_rep |&gt; pull(weight_name)\n      )\n    ) |&gt; \n    # extract the scalar estimate\n    pull(estimate)\n}\n\nIn the code above, sim_rep |&gt; pull(weight_name) takes the data frame sim_rep and extracts the weight variable that is named weight_name. There are other ways to do this also.\nWe can now apply our estimator to get a point estimate with the main sampling weight,\n\nestimate &lt;- estimator(data = sim_rep, weight_name = \"weight\")\n\nwhich yields the point estimate 0.80. We can use the same function to produce the replicate estimates,\n\nreplicate_estimates &lt;- c(\n  estimator(data = sim_rep, weight_name = \"repwt1\"),\n  estimator(data = sim_rep, weight_name = \"repwt2\"),\n  estimator(data = sim_rep, weight_name = \"repwt3\")\n)\n\nyielding the three estimates: 0.67, 0.73, 0.70. In real data, you will want to apply this in a loop because there may be dozens of replicate weights.\nThe standard error of the estimator will be some function of the replicate estimates, likely involving the standard deviation of the replicate estimates. Check with the data distributor for a formula for your case. Once you estimate the standard error, a 95% confidence interval can be constructed with a Normal approximation, as discussed above.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#application-in-the-cps",
    "href": "topics/weights.html#application-in-the-cps",
    "title": "Weights",
    "section": "Application in the CPS",
    "text": "Application in the CPS\nStarting in 2005, the CPS-ASEC samples include 160 replicate weights. If you download replicate weights for many years, the file size will be enormous. We illustrate the use of replicate weights with a question that can be explored with only one year of data: among 25-year olds in 2023, how did the proportion holding four-year college degrees differ across those identifying as male and female?\nWe first load some packages, including the foreach package which will be helpful when looping through replicate weights.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(foreach)\n\nTo answer our research question, we download 2023 CPS-ASEC data including the variables sex, educ, age, the weight variable asecwt, and the replicate weights repwtp*.\n\ncps_data &lt;- read_dta(\"../data_raw/cps_00079.dta\")\n\nWe then define an estimator to use with these data. It accepts a tibble data and a character weight_name identifying the name of the weight variable, and it returns a tibble with two columns: sex and estimate for the estimated proportion with a four-year degree.\n\nestimator &lt;- function(data, weight_name) {\n  data |&gt;\n    # Define focal_weight to hold the selected weight\n    mutate(focal_weight = data |&gt; pull(weight_name)) |&gt;\n    # Restrict to those age 25+\n    filter(age &gt;= 25) |&gt;\n    # Restrict to valid reports of education\n    filter(educ &gt; 1 & educ &lt; 999) |&gt;\n    # Define a binary outcome: a four-year degree\n    mutate(college = educ &gt;= 110) |&gt;\n    # Estimate weighted means by sex\n    group_by(sex) |&gt;\n    summarize(estimate = weighted.mean(\n      x = college,\n      w = focal_weight\n    ))\n}\n\nWe produce a point estimate by applying that estimator with the asecwt.\n\nestimate &lt;- estimator(data = cps_data, weight_name = \"asecwt\")\n\n\n\n# A tibble: 2 × 2\n  sex        estimate\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;\n1 1 [male]      0.369\n2 2 [female]    0.397\n\n\nUsing the foreach package, we apply the estimator 160 times—once with each replicate weight—and use the argument .combine = \"rbind\" to stitch results together by rows.\n\nlibrary(foreach)\nreplicate_estimates &lt;- foreach(r = 1:160, .combine = \"rbind\") %do% {\n  estimator(data = cps_data, weight_name = paste0(\"repwtp\",r))\n}\n\n\n\n# A tibble: 320 × 2\n   sex        estimate\n   &lt;dbl+lbl&gt;     &lt;dbl&gt;\n 1 1 [male]      0.368\n 2 2 [female]    0.396\n 3 1 [male]      0.371\n 4 2 [female]    0.400\n 5 1 [male]      0.371\n 6 2 [female]    0.397\n 7 1 [male]      0.369\n 8 2 [female]    0.397\n 9 1 [male]      0.370\n10 2 [female]    0.398\n# ℹ 310 more rows\n\n\nWe estimate the standard error of our estimator by a formula \\[\\text{StandardError}(\\hat\\tau) = \\sqrt{\\frac{4}{160}\\sum_{r=1}^{160}\\left(\\hat\\tau^*_r - \\hat\\tau\\right)^2}\\] where the formula comes from the survey documentation. We carry out this procedure within groups defined by sex, since we are producing estimate for each sex.\n\nstandard_error &lt;- replicate_estimates |&gt;\n  # Denote replicate estimates as estimate_star\n  rename(estimate_star = estimate) |&gt;\n  # Merge in the point estimate\n  left_join(estimate,\n            by = join_by(sex)) |&gt;\n  # Carry out within groups defined by sex\n  group_by(sex) |&gt;\n  # Apply the formula from survey documentation\n  summarize(standard_error = sqrt(4 / 160 * sum((estimate_star - estimate) ^ 2)))\n\n\n\n# A tibble: 2 × 2\n  sex        standard_error\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [male]          0.00280\n2 2 [female]        0.00291\n\n\nFinally, we combine everything and construct a 95% confidence interval by a Normal approximation.\n\nresult &lt;- estimate |&gt;\n  left_join(standard_error, by = \"sex\") |&gt;\n  mutate(ci_min = estimate - 1.96 * standard_error,\n         ci_max = estimate + 1.96 * standard_error)\n\n\n\n# A tibble: 2 × 5\n  sex        estimate standard_error ci_min ci_max\n  &lt;dbl+lbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1 [male]      0.369        0.00280  0.364  0.375\n2 2 [female]    0.397        0.00291  0.391  0.403\n\n\nWe use ggplot() to visualize the result.\n\nresult |&gt;\n  mutate(sex = as_factor(sex)) |&gt;\n  ggplot(aes(\n    x = sex, \n    y = estimate,\n    ymin = ci_min, \n    ymax = ci_max,\n    label = scales::percent(estimate)\n  )) +\n  geom_errorbar(width = .2) +\n  geom_label() +\n  scale_x_discrete(\n    name = \"Sex\", \n    labels = str_to_title\n  ) +\n  scale_y_continuous(name = \"Proportion with 4-Year College Degree\") +\n  ggtitle(\n    \"Sex Disparities in College Completion\",\n    subtitle = \"Estimates from the 2023 CPS-ASEC among those age 25+\"\n  )\n\n\n\n\n\n\n\n\nWe conclude that those identifying as female are more likely to hold a college degree. Because we can see the confidence intervals generated using the replicate weights, we are reasonably confident in the statistical precision of our point estimates.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/weights.html#footnotes",
    "href": "topics/weights.html#footnotes",
    "title": "Weights",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf we hypothetically drew many complex survey samples from the population in this way, an interval generated this way would contain the true population mean 95% of the time.↩︎",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Weights"
    ]
  },
  {
    "objectID": "topics/sampling.html",
    "href": "topics/sampling.html",
    "title": "Sampling",
    "section": "",
    "text": "[Slides]\nClaims about inequality are often claims about a population, such as all people in the United States. The target population may be so large that full count enumeration—talking to everyone—is prohibitively costly. This lecture introduces probability sampling as a way to learn about a population from a sample of much smaller size.\nWe first discuss the principles of probability sampling using the target population of our class. Then, we discuss how the U.S. Census Bureau carries out the Current Population Survey, a probability sample designed to estimate the unemployment rate in the U.S. and in each state.\nAfter class, you should\n\nread about probability sampling in an online textbook maintained by Statistics Canada. This reading reviews what we covered in lecture and extends it to other ideas in complex survey sampling, such as stratified and cluster sampling\nregister for an account at cps.ipums.org, where we will be accessing data from the Current Population Survey\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Generating Data",
      "Sampling"
    ]
  },
  {
    "objectID": "topics/r_basics.html",
    "href": "topics/r_basics.html",
    "title": "R and RStudio",
    "section": "",
    "text": "This course uses R, RStudio, and Quarto\n\nR is statistical software\nRStudio is a user interface\nQuarto is an authoring framework for reproducible documents\n\nAll are free and open source. To set the software up on your computer, follow the steps in the Prerequisites section of R4DS. To learn about RStudio, visit the User Guide.\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "R and RStudio"
    ]
  },
  {
    "objectID": "topics/data_transformation.html",
    "href": "topics/data_transformation.html",
    "title": "Data transformation",
    "section": "",
    "text": "This exercise examines how income inequality has changed over time in the U.S. We will measure inequality by the 10th, 50th, and 90th percentiles of wage and salary income from 1962 to 2022. We expect that this exercise may take more time than one discussion session.1 You will begin by downloading data and end by making this graph.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#data-access",
    "href": "topics/data_transformation.html#data-access",
    "title": "Data transformation",
    "section": "Data access",
    "text": "Data access\nThis exercise uses data from the Current Population Survey.\n\nRegister for an account at cps.ipums.org\nLog in\nClick “Get Data”\nAdd the following variables to your cart: incwage, educ, wkswork2, age, asecwt\nAdd the 1962–2023 ASEC samples to your cart. Exclude the basic monthly samples\nCreate a data extract\n\nSelect cases to only download people ages 30–45\nChoose to download in Stata (.dta) format\n\nSubmit your extract and download the data!\n\nStore your data in a working directory: a folder on your computer that will hold the data for this exercise.\n\n\n\n\n\n\nTip\n\n\n\nKeep a browser tab open with the IPUMS webpage to easily access full documentation",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#prepare-r-environment",
    "href": "topics/data_transformation.html#prepare-r-environment",
    "title": "Data transformation",
    "section": "Prepare R environment",
    "text": "Prepare R environment\nIn RStudio, create a Quarto document. Save it in your working directory.\nUse the code below to load packages:\n\nlibrary(tidyverse)\nlibrary(haven)\n\nThe haven package allows us to load data in the .dta format designed for Stata. Use read_dta()) and store the data in an object called micro. By default, these data are stored in a tibble.\n\nmicro &lt;- read_dta(\"cps_00077.dta\")\n\n\n\n\n\n\n\nTip\n\n\n\nChange the file name to the name of the file you downloaded\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf R says the file does not exist in your current working directory, you may need to set your working directory by clicking Session -&gt; Set Working Directory -&gt; To Source File Location on a Mac or Tools -&gt; Change Working Directory on Windows.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#get-familiar-with-our-dataset",
    "href": "topics/data_transformation.html#get-familiar-with-our-dataset",
    "title": "Data transformation",
    "section": "Get familiar with our dataset",
    "text": "Get familiar with our dataset\nType micro in the console. What do you see?\n\nHow many rows are there?\nHow many columns?\n\nSome columns such as educ have a numeric code and a label. The code is how IPUMS has stored the data. The label is what the code means. Getting these labels is a benefit of downloading the file in .dta format.\nType View(micro) in the console. This will pop up another tab in RStudio which allows you to scroll through the dataset. You can see that each column name has a description. For instance asecwt is “annual social and economic supplement weight.”",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#filter-to-cases-of-interest",
    "href": "topics/data_transformation.html#filter-to-cases-of-interest",
    "title": "Data transformation",
    "section": "filter() to cases of interest",
    "text": "filter() to cases of interest\n\nIn this step, you will use filter() to convert your micro object to a new object called filtered.\n\nThe filter() function keeps only rows in our dataset that correspond to those we want to study. The examples on the documentation page are especially helpful. The R4DS section is also helpful.\nHere are two ways to use filter() to restrict to people working 50+ weeks per year. One way is to call the filter() function and hand it two arguments\n\n.data = micro is the dataset\nyear == 1962 is a logical condition coded TRUE for observations in 1962\n\n\nfilter(.data = micro, year == 1962)\n\nThe result of this call is a tibble with only the observations from 1962. Another way to do the same operation is with the pipe operator |&gt;\n\nmicro |&gt;\n  filter(year == 1962)\n\nThis approach begins with the data set micro. The pipe operator |&gt; hands this data set on as the first argument to the filter() function in the next line. As before, the second argument is the logical condition year == 1962.\nThe piping approach is often preferable because it reads like a sentence: begin with data, then filter to cases with a given condition. The pipe is also useful\nThe pipe operator |&gt; takes what is on the first line and hands it on as the first argument to the function in the next line. This reads in a sentence: begin with the micro tibble and then filter() to cases with year == 1962. The pipe can also string together many operations, with comments allowed between them:\n\nmicro |&gt;\n  # Restrict to 1962\n  filter(year == 1962) |&gt;\n  # Restrict to ages 40-44\n  filter(age &gt;= 40 & age &lt;= 44)\n\nYour turn. Begin with the micro dataset. Filter to\n\npeople working 50+ weeks per year (check documentation for wkswork2)\nvalid report of incwage greater than 0 and less than 99999998\n\n\n\n\n\n\n\nNote\n\n\n\nfiltered should have 1,350,542 rows and 16 columns.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiltering can be a dangerous business! For example, above we dropped people with missing values of income. But what if the lowest-income people refuse to answer the income question? We often have no choice but to filter to those with valid responses, but you should always read the documentation to be sure you understand who you are dropping and why.\n\n\n\n\n\nfilter() without the pipe\n\n\n\n\nfilter() with the pipe",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "href": "topics/data_transformation.html#group_by-and-summarize-for-subpopulation-summaries",
    "title": "Data transformation",
    "section": "group_by() and summarize() for subpopulation summaries",
    "text": "group_by() and summarize() for subpopulation summaries\n\nIn this step, you will use group_by() and summarize() to convert your mutated object to a new object called summarized.\n\nEach row in our dataset is a person. We want a dataset where each row is a year.\n\nUse group_by() function to group by year. The next operations will automatically be carried out within groups\nUse summarize() to aggregate to the 10th, 50th, and 90th percentiles within each year\n\np10 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.1)\np50 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.5)\np90 = Hmisc::wtd.quantile(x = incwage, weights = asecwt, probs = 0.9)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThat was a new way of calling a package! The Hmisc package has a bunch of miscellaneous functions. To install the package, type install.packages(\"Hmisc\"). One of the functions is wtd.quantile(), which summarizes data with weighted quantiles (e.g., the 10th percentile estimated in a survey with sampling weights). So why call it with Hmisc::wtd.quantile() instead of using library(Hmisc)? When you use library(), you load all the functions in a package. Hmisc is a big package, and some of the functions have the same names as other functions we use in tidyverse. Loading the whole package can create conflicts where two function shave the same name! Instead, the way we’ve written it above tells R to just look in the package for the particular function we’re using at that moment.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe summarized data frame should have 62 rows and 4 columns (year, education categories, p10, p50, and p90).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe are not using the survey weights here. See the next page!\n\n\n\n\n\nSample summaries\n\n\n\n\nWeighted sample summaries",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "href": "topics/data_transformation.html#pivot_longer-to-reshape-data",
    "title": "Data transformation",
    "section": "pivot_longer() to reshape data",
    "text": "pivot_longer() to reshape data\n\nIn this step, you will use pivot_longer() to convert your summarized object to a new object called pivoted. We first explain why, then explain the task.\n\nWe ultimately want to make a ggplot() where income values are placed on the y-axis. We want to plot the 10th, 50th, and 90th percentiles along this axis, distinguished by color. We need them all in one colun! But currently, they are in three columns.\nHere is the task. How our data look:\n\n\n# A tibble: 62 × 4\n   year   p10   p50   p90\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1962  2000  5100  9000\n2  1963  2000  5200  9200\n# ℹ 60 more rows\n\n\nHere we want our data to look:\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10        2000\n2  1962 p50        5100\n3  1962 p90        9000\n4  1963 p10        2000\n5  1963 p50        5200\n6  1963 p90        9200\n# ℹ 180 more rows\n\n\nThis way, we can use year for the x-axis, quantity for color, and value for the y-axis.\nUse pivot_longer() to change the first data frame to the second.\n\nUse the cols argument to tell it which columns will disappear\nUse the names_to argument to tell R that the names of those variables will be moved to a column called quantity\nUse the values_to argument to tell R that the values of those variables will be moved to a column called income",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "href": "topics/data_transformation.html#left_join-an-inflation-adjustment",
    "title": "Data transformation",
    "section": "left_join() an inflation adjustment",
    "text": "left_join() an inflation adjustment\n\nIn this step, you will use left_join() to merge in an inflation adjustment\n\nA dollar in 1962 bought a lot more than a dollar in 2022. We will adjust for inflation using the Consumer Price Index, which tracks the cost of a standard basket of market goods. We already took this index to create a file inflation.csv,\n\ninflation &lt;- read_csv(\"https://info3370.github.io/data/inflation.csv\")\n\n\n\n# A tibble: 62 × 2\n   year inflation_factor\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1962            10.1 \n2  1963             9.95\n3  1964             9.82\n# ℹ 59 more rows\n\n\nThe inflation_factor tells us that $1 in 1962 could buy about as much as $10.10 in 2023. To take a 1962 income and report it in 2023 dollars, we should multiple it by 10.1. We need to join our data\n\n\n# A tibble: 186 × 3\n   year quantity income\n  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1  1962 p10        2000\n2  1962 p50        5100\n3  1962 p90        9000\n# ℹ 183 more rows\n\n\ntogether with inflation.csv by the linking variable year. Use left_join() to merge inflation_factor onto the dataset pivoted. Below is a hypothetical example for the structure.\n\n# Hypothetical example\njoined &lt;- data_A |&gt;\n  left_join(\n    data_B,\n    by = join_by(key_variable_in_A_and_B)\n  )",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "href": "topics/data_transformation.html#mutate-to-adjust-for-inflation",
    "title": "Data transformation",
    "section": "mutate() to adjust for inflation",
    "text": "mutate() to adjust for inflation\n\nIn this step, you will use mutate() to multiple income by the inflation_factor\n\nThe mutate() function modifies columns. It can overwrite existing columns or create new columns at the right of the data set. The new variable is some transformation of the old variables.\n\n# Hypothetical example\nold_data |&gt;\n  mutate(new_variable = old_variable_1 + old_variable_2)\n\nUse mutate() to modify income so that it takes the values income * inflation_factor.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#ggplot-to-visualize",
    "href": "topics/data_transformation.html#ggplot-to-visualize",
    "title": "Data transformation",
    "section": "ggplot() to visualize",
    "text": "ggplot() to visualize\nNow make a ggplot() where\n\nyear is on the x-axis\nincome is on the y-axis\nquantity is denoted by color\n\nDiscuss. What do you see in this plot?",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#finished-early",
    "href": "topics/data_transformation.html#finished-early",
    "title": "Data transformation",
    "section": "Finished early?",
    "text": "Finished early?\nIf you are finished early, you could\n\nincorporate the educ variable in your plot. You might want to group by those who do and do not hold college degrees, perhaps using facet_grid()\ntry geom_histogram() for people’s incomes in a specific year\nexplore IPUMS-CPS for other variables and begin your own visualization",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/data_transformation.html#footnotes",
    "href": "topics/data_transformation.html#footnotes",
    "title": "Data transformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to past TA Abby Sachar for designing the base of this exercise.↩︎",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Data transformation"
    ]
  },
  {
    "objectID": "topics/asking_questions.html",
    "href": "topics/asking_questions.html",
    "title": "Asking Questions",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Asking Questions"
    ]
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "Check the post pinned on Ed Discussion to see any week-specific changes to the schedule.\n\n\n\nPerson\nTime\nLocation\n\n\n\n\nStacy\nM 12–1pm\nRhodes 402\n\n\nJoanne\nM 1–2pm\nRhodes 408\n\n\nChang\nM 3–4pm\nRhodes 408\n\n\nIan\nT 10–11am\nGates 223\n\n\nLiz\nW 12:30–1:30pm\nRhodes 402\n\n\nDaniel\nW 1–2pm\nRhodes 406\n\n\nFede\nW 2–3pm\nRhodes 406\n\n\nOlaf\nW 3–4pm\nRhodes 406\n\n\nJonathan\nTh 11am–12pm\nRhodes 412\n\n\nAkira\nTh 3:30–4:30pm\nZoom\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "forms.html",
    "href": "forms.html",
    "title": "Forms",
    "section": "",
    "text": "If you are missing class, here is a form where you can tell us!\n\nAssignment extension form to request an assignment extension that does not count against flex days under exceptional circumstances\nExcused absence form. To request an excused absence from lecture or discussion\nLecture make-up form. For lectures that are missed which we haven’t excused\nFor non-excused absences from discussion, you should do the discussion activity on your own and submit in the Canvas assignment associated with that discussion\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/pset1.html",
    "href": "assignments/pset1.html",
    "title": "Problem Set 1: Visualization",
    "section": "",
    "text": "Due: 5pm on Wednesday, January 31.\n\n\n\n\n\n\nBefore you start\n\n\n\nCreate an anonymous identifier for yourself here! Want to see how you’ll be evaluated? Check out the rubric\n\n\nStudent identifer: [type your anonymous identifier here]\n\nUse this .qmd template to complete the problem set\nIn Canvas, you will upload the PDF produced by your .qmd file\nPut your identifier above, not your name! We want anonymous grading to be possible\n\nThis problem set involves both data analysis and reading.\n\nData analysis\nThis problem set uses the data lifeCourse.csv.\n\nlibrary(tidyverse)\nlibrary(scales)\nlifeCourse &lt;- read_csv(\"https://info3370.github.io/data/lifeCourse.csv\")\n\nThe data contain life course earnings profiles for four cohorts of American workers: those born in 1940, 1950, 1960, and 1970. Each row contains a summary of the annual earnings distribution for a particular birth cohort at a particular age, among the subgroup with a particular level of education. To prepare these data, we aggregated microdata from the Current Population Survey, provided through the Integrated Public Use Microdata Series.\nThe data contain five variables.\n\nquantity is the metric by which the earnings distribution is summarized: 10th, 50th, or 90th percentile\neducation is the educational subgroup being summarized: College Degree, Less than College\ncohort is the cohort (people with a given birth year) to which these data apply: 1940, 1950, 1960, 1970\nage is the age at which earnings were measured: 30–45\nincome is the value for the given earnings percentile in the given subgroup. Income values are provided in 2022 dollars\n\n\n\n1. Visualize (25 points)\nUse ggplot to visualize these data. To denote the different trajectories,\n\nmake your plot using geom_point() or geom_line()\nuse the x-axis for age\nuse the y-axis for income\nuse color for quantity\nuse facet_grid to make a panel of facets where each row is an education value and each column is a cohort value\n\nYou should prepare the graph as though you were going to publish it. Modify the axis titles so that a reader would know what is on the axis. Use appropriate capitalization in all labels. Try using the label_dollar() function from the scales package so that the y-axis uses dollar values.\nYour code should be well-formatted as defined by R4DS. In your produced PDF, no lines of code should run off the page.\nMany different graphs can be equally correct. You will be evaluated by\n\nhaving publication-ready graph aesthetics\ncode that follows style conventions\n\n\n# your code goes here\n\n\n\n2. Interpret (10 points)\nWrite 2-3 sentences summarizing the trends that you see in the data.\n2.1 (3 points). Focus on those born in 1970. For those with a college degree, how do the top and bottom of the income distribution change over the life course?\n\nType your answer here.\n\n2.2 (3 points). Focus on those born in 1970. How does the pattern differ for those without college degres differ from your answer in 2.1?\n\nType your answer here.\n\n2.3 (4 points). How do the patterns you identified in 2.1 and 2.2 change from the 1940 to the 1970 cohort?\n\nType your answer here.\n\n\n\n3. Connect to reading (15 points)\nRead p. 1–7 of following paper. Stop before the section “Analytic Framework for Decomposing Inequality.”\n\nCheng, Siwei. 2021. The shifting life course patterns of wage inequality.. Social Forces 100(1):1–28.\n\nOur data are not the same as Cheng’s. But our analysis is able to reproduce many of her findings. Answer each question in two sentences or less.\nCheng discusses period trends, cohort trends, and age trends.\n3.1 (3 points) Which dimension of your graph shows a cohort trend?\n\nType your answer here.\n\n3.2 (3 points) Which dimension of your graph shows an age trend?\n\nType your answer here.\n\n3.3 (3 points) Cheng discusses education-based cumulative advantage. Describe how you see this in your graph.\n\nType your answer here.\n\n3.4 (3 points) Cheng discusses within-education trajectory heterogeneity. Describe how your graph shows heterogeneity of outcomes within educational categories.\n\nType your answer here.\n\n3.5 (3 points) Cheng discusses wage volatility: how wages rise and fall over time for a given person. Why is our data (the Current Population Survey) the wrong dataset to study wage volatility?\n\nType your answer here.\n\n\n\nGrad question: Model-based estimates\nThis question assumes familiarity with Ordinary Least Squares.\n\nFor graduate students, this question is worth 20 points.\nFor undergraduate students, this question is optional and worth 0 points.\n\nThe data contain nonparametric estimates that contain some noise: the data points provided partly reflect random variation because they are estimatd in a sample.\nModel-based estimates reduce noise by pooling information across observations, at the cost of introducing assumptions. Fit an OLS model to the data using age as a numeric variable and education, cohort, and quantity as factor variables. Interact all of these predictors with each other.\n\nfit &lt;- lm(\n  income ~ age * factor(cohort) * education * quantity,\n  data = lifeCourse\n)\n\nEffectively, this estimates a best-fit line through each set of points depicted in your original figure. For each observation, store a prediction from this model (see predict()).\nRe-create your plot from (1) using\n\ngeom_point() for the nonparametric estimates (as above)\ngeom_line() for your model-based predicted values\n\n\n\nComputing environment\nLeave this at the bottom of your file, and it will record information such as your operating system, R version, and package versions. This is helpful for resolving any differences in results across people.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] scales_1.2.1    lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.3     purrr_1.0.1     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] bit_4.0.5         gtable_0.3.3      jsonlite_1.8.4    crayon_1.5.2     \n [5] compiler_4.3.1    renv_1.0.3        tidyselect_1.2.0  parallel_4.3.1   \n [9] yaml_2.3.7        fastmap_1.1.1     R6_2.5.1          generics_0.1.3   \n[13] knitr_1.43        htmlwidgets_1.6.4 munsell_0.5.0     pillar_1.9.0     \n[17] tzdb_0.4.0        rlang_1.1.1       utf8_1.2.3        stringi_1.7.12   \n[21] xfun_0.40         bit64_4.0.5       timechange_0.2.0  cli_3.6.1        \n[25] withr_2.5.0       magrittr_2.0.3    digest_0.6.33     grid_4.3.1       \n[29] vroom_1.6.3       rstudioapi_0.15.0 hms_1.1.3         lifecycle_1.0.3  \n[33] vctrs_0.6.3       evaluate_0.21     glue_1.6.2        fansi_1.0.4      \n[37] colorspace_2.1-0  rmarkdown_2.25    tools_4.3.1       pkgconfig_2.0.3  \n[41] htmltools_0.5.7  \n\n\n\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The culmination of the course is a group research project. You will\n\ncreate your own research question\ndownload your own data\nvisualize the data\ninterpret your findings\n\nMore details on the project will come later in the semester.\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/pset0.html",
    "href": "assignments/pset0.html",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "",
    "text": "Due: 5pm on Wednesday, January 24.\nIn this exercise, you will install R, RStudio and Quarto. The exercise will ensure that they are set up correctly so that everybody is ready to dive into coursework.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#install-software-requirements",
    "href": "assignments/pset0.html#install-software-requirements",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Install Software Requirements",
    "text": "Install Software Requirements\nYou should follow the instructions in R4DS to:\n\nInstall R\nInstall RStudio\n\nOnce R and RStudio are successfully installed, this also means that Quarto is successfully installed because it comes bundled with RStudio.\nFinally, you will need to open RStudio, select the Terminal tab, and execute the following command: quarto install tinytex as demonstrated in the image below.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#download-problem-set",
    "href": "assignments/pset0.html#download-problem-set",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Download Problem Set",
    "text": "Download Problem Set\nNext, download this problem set which is a Quarto Markdown Document.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#render-the-problem-set",
    "href": "assignments/pset0.html#render-the-problem-set",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Render the Problem Set",
    "text": "Render the Problem Set\nFinally, open the downloaded problem set in RStudio. You should then click the “Render” button as shown in the image below.\n This will run the R code and combine the output with the text in the document into a PDF. By default, this PDF will be output in the same directory that you saved pset0.qmd in. You will then submit the PDF on Canvas.",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "assignments/pset0.html#issues",
    "href": "assignments/pset0.html#issues",
    "title": "Problem Set 0: Setting up R, RStudio, and Quarto",
    "section": "Issues",
    "text": "Issues\nIf you run into issues while attempting to render the Problem Set, be sure to open a question on the Ed Discussion!",
    "crumbs": [
      " ",
      "Assignments",
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "due_dates.html",
    "href": "due_dates.html",
    "title": "Due Dates",
    "section": "",
    "text": "Due date\nAssignment\n\n\n\n\nWed Jan 24 at 5pm\nProblem Set 0\n\n\nWed Jan 31 at 5pm\nProblem Set 1\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Studying Social Inequality with Data Science",
    "section": "",
    "text": "Together, we will generate new knowledge about social inequality using the tools of data science",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Studying Social Inequality with Data Science",
    "section": "Learning goals",
    "text": "Learning goals\nAs a result of participating in this course, students will be able to\n\nvisualize economic inequality with graphs that summarize survey data\nconnect theories about inequality to quantitative empirical evidence\nevaluate the effects of hypothetical interventions to reduce inequality\nconduct data analysis using the R programming language",
    "crumbs": [
      " ",
      "Home"
    ]
  },
  {
    "objectID": "topics/about.html",
    "href": "topics/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "topics/class.html",
    "href": "topics/class.html",
    "title": "Class",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing inequality",
      "Class"
    ]
  },
  {
    "objectID": "topics/gender.html",
    "href": "topics/gender.html",
    "title": "Gender",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing inequality",
      "Gender"
    ]
  },
  {
    "objectID": "topics/race.html",
    "href": "topics/race.html",
    "title": "Race",
    "section": "",
    "text": "TODO\n\n\n\n Back to top",
    "crumbs": [
      " ",
      "Topics",
      "Describing inequality",
      "Race"
    ]
  },
  {
    "objectID": "topics/visualization.html",
    "href": "topics/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "Prerequisites. You should first install R and RStudio as described in the R4DS Prerequisites. If you are unfamiliar with the layout of RStudio, see the User Guide.\nIn this discussion section, we will",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-the-environment",
    "href": "topics/visualization.html#prepare-the-environment",
    "title": "Visualization",
    "section": "Prepare the environment",
    "text": "Prepare the environment\nOpen a new R Script by clicking the button at the top left of RStudio. Save your R Script in a folder you will use for this exercise.\nPaste the code below into your R Script. Place your cursor within the line and hit CMD + Enter or CTRL + Enter to run the code and load the tidyverse package.\n\nlibrary(tidyverse)\n\nYou will see action in the console. You have added some functionality to R for this session!\nThe data can be loaded from the course website with the line below.\n\ndata &lt;- read_csv(file = \"https://info3370.github.io/data/jencks_table1.csv\")\n\nWhen you run this code, the object data will appear in your environment pane.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#explore-the-data",
    "href": "topics/visualization.html#explore-the-data",
    "title": "Visualization",
    "section": "Explore the data",
    "text": "Explore the data\nType data in your console. You can see the data!\n\ncountry country name\nratio ratio of 90th to 10th percentile of household income. You can think of this as how many dollars a high-income household receives for each dollar that a low-income household receives\ngdp Gross Domestic Product Per Capita, expressed as a proportion of U.S. GDP\nlife_expectancy life expectancy at birth\n\nFor details on the data, see Jencks (2002) Table 1.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#produce-a-graph",
    "href": "topics/visualization.html#produce-a-graph",
    "title": "Visualization",
    "section": "Produce a graph",
    "text": "Produce a graph\nWe are ready to produce a graph! The code below will produce a simple graph.\n\ndata |&gt;\n  ggplot(mapping = aes(x = ratio, y = gdp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nLet’s break this code down into pieces\n\ndata tells R to start with the object data\n|&gt; is called the pipe operator. It passes the data object down to the function in the next line\nggplot() is a function that creates a plot environment\nthe argument mapping = aes(x = ratio, y = gdp) tells ggplot() how variables in the data will correspond to elements of the plot. We will visualize ratio on the x-axis and gdp on the y-axis\n+ tells ggplot() we will add a new layer on the next line\ngeom_point() tells ggplot() to add a layer of points to the graph",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#customizing-your-graph",
    "href": "topics/visualization.html#customizing-your-graph",
    "title": "Visualization",
    "section": "Customizing your graph",
    "text": "Customizing your graph\nIn your group, create additional layers with additional lines connected by +. Be creative! Here are some ideas:\n\nlabel the axes with scale_x_continuous(name = \"your text here\") and scale_y_continuous(name = \"your text here\")\nlabel countries using geom_text or geom_text_repel, with the aesthetic label = country\n\nThere are many possible graphs to make. An example is below.",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#interpret-your-graph",
    "href": "topics/visualization.html#interpret-your-graph",
    "title": "Visualization",
    "section": "Interpret your graph",
    "text": "Interpret your graph\nOnce you are happy with your graph,\n\nwrite a few sentences explaining your graph\ndiscuss what questions you would like to ask next",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/visualization.html#prepare-a-quarto-report",
    "href": "topics/visualization.html#prepare-a-quarto-report",
    "title": "Visualization",
    "section": "Prepare a Quarto report",
    "text": "Prepare a Quarto report\nCreate a new Quarto document. Put your R code and interpretation into that document. Upload to Canvas!",
    "crumbs": [
      " ",
      "Topics",
      "Working with Data",
      "Visualization"
    ]
  },
  {
    "objectID": "topics/welcome.html",
    "href": "topics/welcome.html",
    "title": "Welcome!",
    "section": "",
    "text": "[Slides]\nAmerica is more unequal by many measures than other industrialized nations. Should we be concerned? In today’s reading, Jencks discusses how quantitative empirical evidence on inequality relates to arguments about the amount of inequality that is desirable.\nAfter our discussion in class, read p. 49–53 of this paper:\nThe paper’s engagement with both normative arguments and quantitative evidence is an example we will follow this semester.\nUltimately, this class is creative: you will produce new evidence about a question of your choosing. To get there, we first need to learn a bit about population data and how to work with those data.",
    "crumbs": [
      " ",
      "Topics",
      "Welcome!"
    ]
  },
  {
    "objectID": "topics/welcome.html#summary-video-what-we-covered-today",
    "href": "topics/welcome.html#summary-video-what-we-covered-today",
    "title": "Welcome!",
    "section": "Summary video: What we covered today",
    "text": "Summary video: What we covered today",
    "crumbs": [
      " ",
      "Topics",
      "Welcome!"
    ]
  }
]